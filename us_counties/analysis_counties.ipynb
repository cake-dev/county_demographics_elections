{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas:\n",
    "\n",
    "change in who a county votes for (flips or not) is spatially correlated / informed \n",
    "\n",
    "spatial correlation on residuals from a model that predicts winner in each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full census election data geojson\n",
    "census_election_data = gpd.read_file('../../data/county_demographics_with_elections_2012_2024.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a new geojson file\n",
    "census_election_data.to_file('../../data/county_demographics_with_elections_2012_2024_resave.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_election_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the relgion csv\n",
    "religion_data = pd.read_csv('religion_groups_county.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with all NaN in Adherents\tAdherents as % of Total Adherents\tAdherents as % of Total Population\n",
    "religion_data = religion_data.dropna(subset=['Adherents', 'Adherents as % of Total Adherents', 'Adherents as % of Total Population'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Congregations column (weird NaN bug)\n",
    "religion_data = religion_data.drop(columns=['Congregations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all values in the religion data to be percentages instead of strings with percentage signs.  also renmove commas from numbers and convert to integers\n",
    "# Adherents as % of Total Adherents, Adherents as % of Total Population, Adherents\n",
    "religion_data['Adherents as % of Total Adherents'] = religion_data['Adherents as % of Total Adherents'].str.replace('%', '').str.replace(',', '').astype(float) / 100\n",
    "religion_data['Adherents as % of Total Population'] = religion_data['Adherents as % of Total Population'].str.replace('%', '').str.replace(',', '').astype(float) / 100\n",
    "religion_data['Adherents'] = religion_data['Adherents'].str.replace(',', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 5 group names with the most adherents\n",
    "religion_data.groupby('Group Name')['Adherents'].sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data.groupby('Group Name')['Adherents'].sum().sort_values(ascending=False).head(20).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data['Group Name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "145119464/161009516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where the group name is not in the top 20:\n",
    "# 'Catholic Church', 'Non-denominational Christian Churches',\n",
    "#        'Southern Baptist Convention', 'United Methodist Church',\n",
    "#        'Church of Jesus Christ of Latter-day Saints', 'Muslim Estimate',\n",
    "#        'Evangelical Lutheran Church in America', 'Assemblies of God',\n",
    "#        'Jehovah's Witnesses', 'National Missionary Baptist Convention, Inc.',\n",
    "#        'Lutheran Church--Missouri Synod', 'Episcopal Church',\n",
    "#        'National Baptist Convention, USA, Inc.',\n",
    "#        'Presbyterian Church (U.S.A.)', 'Churches of Christ',\n",
    "#        'Christian Churches and Churches of Christ',\n",
    "#        'Seventh-day Adventist Church', 'American Baptist Churches in the USA',\n",
    "#        'African Methodist Episcopal Church', 'Orthodox Judaism'],\n",
    "religion_data = religion_data[religion_data['Group Name'].isin([\n",
    "    'Catholic Church', 'Non-denominational Christian Churches',\n",
    "    'Southern Baptist Convention', 'United Methodist Church',\n",
    "    'Church of Jesus Christ of Latter-day Saints', 'Muslim Estimate',\n",
    "    'Evangelical Lutheran Church in America', 'Assemblies of God',\n",
    "    \"Jehovah's Witnesses\", 'National Missionary Baptist Convention, Inc.',\n",
    "    'Lutheran Church--Missouri Synod', 'Episcopal Church',\n",
    "    'National Baptist Convention, USA, Inc.',\n",
    "    'Presbyterian Church (U.S.A.)', 'Churches of Christ',\n",
    "    'Christian Churches and Churches of Christ',\n",
    "    'Seventh-day Adventist Church', 'American Baptist Churches in the USA',\n",
    "    'African Methodist Episcopal Church', 'Orthodox Judaism'\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(religion_data['Group Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_religion_data(df):\n",
    "    \"\"\"\n",
    "    Transform religion data from long to wide format with one row per county\n",
    "    and separate columns for each religion's metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe with religion data in long format\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Transformed dataframe with one row per county\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create base identifier columns for counties\n",
    "    id_cols = ['FIPS', 'State Name', 'County Name']\n",
    "    \n",
    "    # Create three separate pivot tables for each metric\n",
    "    adherents_pivot = df.pivot(\n",
    "        index=id_cols,\n",
    "        columns='Group Name',\n",
    "        values='Adherents'\n",
    "    ).add_suffix('_adherents')\n",
    "    \n",
    "    pct_adherents_pivot = df.pivot(\n",
    "        index=id_cols,\n",
    "        columns='Group Name',\n",
    "        values='Adherents as % of Total Adherents'\n",
    "    ).add_suffix('_percent_adherents_of_total_adherents')\n",
    "    \n",
    "    pct_pop_pivot = df.pivot(\n",
    "        index=id_cols,\n",
    "        columns='Group Name',\n",
    "        values='Adherents as % of Total Population'\n",
    "    ).add_suffix('_percent_adherents_of_total_population')\n",
    "    \n",
    "    # Combine all pivot tables\n",
    "    result = pd.concat([\n",
    "        adherents_pivot,\n",
    "        pct_adherents_pivot,\n",
    "        pct_pop_pivot\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Reset index to make FIPS, State Name, and County Name regular columns\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    # Clean up column names and add 'religion_' prefix to all religion-related columns\n",
    "    result.columns = [\n",
    "        ('religion_' + col.replace(' ', '_').replace(\"'\", '').replace(',', '').replace('-', '_').lower()\n",
    "         if col not in id_cols else col.replace(' ', '_').lower())\n",
    "        for col in result.columns\n",
    "    ]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_transformed = transform_religion_data(religion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_transformed.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with state_name == 'District of Columbia', 'Alaska', 'Hawaii', 'Puerto Rico'\n",
    "religion_transformed = religion_transformed[~religion_transformed['state_name'].isin(['District of Columbia', 'Alaska', 'Hawaii', 'Puerto Rico'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change fips to float\n",
    "religion_transformed['fips'] = religion_transformed['fips'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge religion_transformed with census_election_data on fips and FIPS Code,respectively\n",
    "merged_data = pd.merge(census_election_data, religion_transformed, left_on='FIPS Code', right_on='fips', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_race_percentages(df):\n",
    "    \"\"\"\n",
    "    Add percentage columns for each race category with 'pct_' prefix.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing race population data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with new percentage columns added\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define the years and categories\n",
    "    years = [2012, 2016, 2020]\n",
    "    categories = [\n",
    "        'American Indian and Alaska Native alone',\n",
    "        'Asian alone',\n",
    "        'Black or African American alone',\n",
    "        'Native Hawaiian and Other Pacific Islander alone',\n",
    "        'Some other race alone',\n",
    "        'Two or more races:',\n",
    "        'White alone'\n",
    "    ]\n",
    "    \n",
    "    # Calculate percentages for each year and category for each county\n",
    "    # iterate over all rows\n",
    "    for i, row in df.iterrows():\n",
    "        # iterate over all years\n",
    "        for year in years:\n",
    "            # iterate over all categories\n",
    "            for category in categories:\n",
    "                # calculate the percentage\n",
    "                total = row[f'Total:_{year}']\n",
    "                value = row[f'{category}_{year}']\n",
    "                pct = round((value / total) , 4)\n",
    "                # add the percentage to the DataFrame\n",
    "                df.at[i, f'pct_{category}_{year}'] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "df2 = add_race_percentages(census_election_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column called 'president_winner_{year}' that contains the party of the candidate that won the presidential election in that year\n",
    "\n",
    "def add_president_winner(df):\n",
    "    \"\"\"\n",
    "    Add a column containing the party of the candidate that won the presidential election in that year.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing election data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with new column added\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define the years\n",
    "    years = [2012, 2016, 2020]\n",
    "    \n",
    "    # Define the election winners\n",
    "    winners = {\n",
    "        2012: 'DEM',\n",
    "        2016: 'GOP',\n",
    "        2020: 'DEM'\n",
    "    }\n",
    "    \n",
    "    # Add a column for each year containing the party of the election winner\n",
    "    for year in years:\n",
    "        df[f'president_winner_{year}'] = winners[year]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_president_winner(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_election_data['president_winner_2024']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split off data for election results and racial demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_columns(df):\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define the columns to split off\n",
    "    columns = [\n",
    "        'geometry',\n",
    "        'county_name',\n",
    "        'state_name',\n",
    "        'total_votes_2012',\n",
    "        'votes_dem_2012',\n",
    "        'votes_gop_2012',\n",
    "        'per_dem_2012',\n",
    "        'per_gop_2012',\n",
    "        'diff_2012',\n",
    "        'per_point_diff_2012',\n",
    "        'total_votes_2016',\n",
    "        'votes_dem_2016',\n",
    "        'votes_gop_2016',\n",
    "        'per_dem_2016',\n",
    "        'per_gop_2016',\n",
    "        'diff_2016',\n",
    "        'per_point_diff_2016',\n",
    "        'votes_gop_2020',\n",
    "        'votes_dem_2020',\n",
    "        'total_votes_2020',\n",
    "        'diff_2020',\n",
    "        'per_gop_2020',\n",
    "        'per_dem_2020',\n",
    "        'per_point_diff_2020',\n",
    "        'winner_2012',\n",
    "        'winner_2016',\n",
    "        'winner_2020',\n",
    "        'president_winner_2012',\n",
    "        'president_winner_2016',\n",
    "        'president_winner_2020',\n",
    "        'Total:_2012',\n",
    "        'pct_American Indian and Alaska Native alone_2012',\n",
    "        'pct_Asian alone_2012',\n",
    "        'pct_Black or African American alone_2012',\n",
    "        'pct_Native Hawaiian and Other Pacific Islander alone_2012',\n",
    "        'pct_Some other race alone_2012',\n",
    "        'pct_Two or more races:_2012',\n",
    "        'pct_White alone_2012',\n",
    "        'Total:_2016',\n",
    "        'pct_American Indian and Alaska Native alone_2016',\n",
    "        'pct_Asian alone_2016',\n",
    "        'pct_Black or African American alone_2016',\n",
    "        'pct_Native Hawaiian and Other Pacific Islander alone_2016',\n",
    "        'pct_Some other race alone_2016',\n",
    "        'pct_Two or more races:_2016',\n",
    "        'pct_White alone_2016',\n",
    "        'Total:_2020',\n",
    "        'pct_American Indian and Alaska Native alone_2020',\n",
    "        'pct_Asian alone_2020',\n",
    "        'pct_Black or African American alone_2020',\n",
    "        'pct_Native Hawaiian and Other Pacific Islander alone_2020',\n",
    "        'pct_Some other race alone_2020',\n",
    "        'pct_Two or more races:_2020',\n",
    "        'pct_White alone_2020',\n",
    "        'FIPS Code'\n",
    "    ]\n",
    "\n",
    "    # Split off the columns\n",
    "    df = df[columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_and_election = split_columns(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Election voting analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_and_visualize_elections(df):\n",
    "    \"\"\"\n",
    "    One-line function to analyze and visualize county importance in presidential elections.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing county election data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (analysis_results, fig) containing dictionary of analysis results and matplotlib figure\n",
    "    \"\"\"\n",
    "    # Calculate metrics (reusing previous analysis logic)\n",
    "    df['avg_total_votes'] = df[['total_votes_2012', 'total_votes_2016', 'total_votes_2020']].mean(axis=1)\n",
    "    df['margin_shift_2012_2016'] = df['per_point_diff_2016'] - df['per_point_diff_2012']\n",
    "    df['margin_shift_2016_2020'] = df['per_point_diff_2020'] - df['per_point_diff_2016']\n",
    "    df['margin_volatility'] = np.abs(df['margin_shift_2012_2016']) + np.abs(df['margin_shift_2016_2020'])\n",
    "    df['avg_margin'] = np.abs(df[['per_point_diff_2012', 'per_point_diff_2016', 'per_point_diff_2020']].mean(axis=1))\n",
    "    df['party_changes'] = ((df['winner_2016'] != df['winner_2012']).astype(int) + \n",
    "                          (df['winner_2020'] != df['winner_2016']).astype(int))\n",
    "    df['electoral_power_index'] = ((df['avg_total_votes'] / df['avg_total_votes'].max()) * \n",
    "                                 (1 - (np.abs(df['avg_margin']) / 100)))\n",
    "\n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Subplot 1: Electoral Power Bubble Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(df['avg_margin'], np.log10(df['avg_total_votes']), \n",
    "               s=df['electoral_power_index']*1000, alpha=0.5)\n",
    "    plt.xlabel('Average Margin (%)')\n",
    "    plt.ylabel('Log10(Average Total Votes)')\n",
    "    plt.title('County Electoral Power\\n(Bubble size = Electoral Power Index)')\n",
    "    \n",
    "    # Add annotations for top 10 most important counties\n",
    "    top_10 = df.nlargest(10, 'electoral_power_index')\n",
    "    for _, county in top_10.iterrows():\n",
    "        plt.annotate(f\"{county['county_name']}, {county['state_name']}\", \n",
    "                    (county['avg_margin'], np.log10(county['avg_total_votes'])))\n",
    "    \n",
    "    # Subplot 2: Swing County Analysis\n",
    "    plt.subplot(2, 2, 2)\n",
    "    swing_counties = df[df['party_changes'] > 0]\n",
    "    sns.scatterplot(data=swing_counties, x='margin_volatility', y='avg_total_votes', \n",
    "                   hue='party_changes', size='avg_total_votes', sizes=(100, 1000))\n",
    "    plt.title('Swing County Analysis')\n",
    "    plt.xlabel('Margin Volatility')\n",
    "    plt.ylabel('Average Total Votes')\n",
    "    \n",
    "    # Subplot 3: Demographic Change Impact\n",
    "    plt.subplot(2, 2, 3)\n",
    "    demographic_cols_2020 = [col for col in df.columns if col.startswith('pct_') and col.endswith('_2020')]\n",
    "    demographic_cols_2012 = [col.replace('2020', '2012') for col in demographic_cols_2020]\n",
    "    total_change = pd.DataFrame()\n",
    "    for col_2012, col_2020 in zip(demographic_cols_2012, demographic_cols_2020):\n",
    "        total_change[f'change_{col_2012}'] = df[col_2020] - df[col_2012]\n",
    "    df['demographic_change_magnitude'] = np.abs(total_change).sum(axis=1)\n",
    "    \n",
    "    plt.scatter(df['demographic_change_magnitude'], df['margin_volatility'], \n",
    "               c=df['electoral_power_index'], cmap='viridis')\n",
    "    plt.colorbar(label='Electoral Power Index')\n",
    "    plt.xlabel('Magnitude of Demographic Change')\n",
    "    plt.ylabel('Margin Volatility')\n",
    "    plt.title('Demographic Change vs. Electoral Volatility')\n",
    "    \n",
    "    # Subplot 4: County Categories\n",
    "    plt.subplot(2, 2, 4)\n",
    "    competitive_threshold = 0.1\n",
    "    df['category'] = 'Stable'\n",
    "    df.loc[df['party_changes'] > 0, 'category'] = 'Swing'\n",
    "    df.loc[(np.abs(df['avg_margin']) < competitive_threshold) & \n",
    "           (df['avg_total_votes'] > df['avg_total_votes'].quantile(0.75)), 'category'] = 'Battleground'\n",
    "    df.loc[df['electoral_power_index'] > df['electoral_power_index'].quantile(0.95), 'category'] = 'High Impact'\n",
    "    \n",
    "    category_counts = df['category'].value_counts()\n",
    "    plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Distribution of County Categories')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Compile analysis results\n",
    "    results = {\n",
    "        'high_impact_counties': df.nlargest(20, 'electoral_power_index')[\n",
    "            ['county_name', 'state_name', 'electoral_power_index', 'avg_total_votes', 'avg_margin']\n",
    "        ],\n",
    "        'swing_counties': df[df['party_changes'] > 0].sort_values(\n",
    "            by=['party_changes', 'avg_total_votes'], ascending=[False, False]\n",
    "        )[['county_name', 'state_name', 'party_changes', 'margin_volatility']].head(20),\n",
    "        'battleground_counties': df[\n",
    "            (np.abs(df['avg_margin']) < competitive_threshold) &\n",
    "            (df['avg_total_votes'] > df['avg_total_votes'].quantile(0.75))\n",
    "        ][['county_name', 'state_name', 'avg_margin', 'avg_total_votes']].head(20),\n",
    "        'demographic_shift_counties': df.nlargest(20, 'demographic_change_magnitude')[\n",
    "            ['county_name', 'state_name', 'demographic_change_magnitude']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return results, fig\n",
    "\n",
    "# Usage example:\n",
    "# results, fig = analyze_and_visualize_elections(your_dataframe)\n",
    "# fig.show()\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Electoral analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_electoral_votes(df):\n",
    "    \"\"\"\n",
    "    Adds electoral vote counts for each state based on 2020/2024 electoral college allocation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing a state_name column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with added electoral_votes column\n",
    "    \"\"\"\n",
    "    \n",
    "    electoral_votes_dict = {\n",
    "        'Alabama': 9,\n",
    "        'Alaska': 3,\n",
    "        'Arizona': 11,\n",
    "        'Arkansas': 6,\n",
    "        'California': 54,\n",
    "        'Colorado': 10,\n",
    "        'Connecticut': 7,\n",
    "        'Delaware': 3,\n",
    "        'District of Columbia': 3,\n",
    "        'Florida': 30,\n",
    "        'Georgia': 16,\n",
    "        'Hawaii': 4,\n",
    "        'Idaho': 4,\n",
    "        'Illinois': 19,\n",
    "        'Indiana': 11,\n",
    "        'Iowa': 6,\n",
    "        'Kansas': 6,\n",
    "        'Kentucky': 8,\n",
    "        'Louisiana': 8,\n",
    "        'Maine': 4,\n",
    "        'Maryland': 10,\n",
    "        'Massachusetts': 11,\n",
    "        'Michigan': 15,\n",
    "        'Minnesota': 10,\n",
    "        'Mississippi': 6,\n",
    "        'Missouri': 10,\n",
    "        'Montana': 4,\n",
    "        'Nebraska': 5,\n",
    "        'Nevada': 6,\n",
    "        'New Hampshire': 4,\n",
    "        'New Jersey': 14,\n",
    "        'New Mexico': 5,\n",
    "        'New York': 28,\n",
    "        'North Carolina': 16,\n",
    "        'North Dakota': 3,\n",
    "        'Ohio': 17,\n",
    "        'Oklahoma': 7,\n",
    "        'Oregon': 8,\n",
    "        'Pennsylvania': 19,\n",
    "        'Rhode Island': 4,\n",
    "        'South Carolina': 9,\n",
    "        'South Dakota': 3,\n",
    "        'Tennessee': 11,\n",
    "        'Texas': 40,\n",
    "        'Utah': 6,\n",
    "        'Vermont': 3,\n",
    "        'Virginia': 13,\n",
    "        'Washington': 12,\n",
    "        'West Virginia': 4,\n",
    "        'Wisconsin': 10,\n",
    "        'Wyoming': 3\n",
    "    }\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_with_ev = df.copy()\n",
    "    \n",
    "    # Add electoral votes column\n",
    "    df_with_ev['electoral_votes'] = df_with_ev['state_name'].map(electoral_votes_dict)\n",
    "    \n",
    "    # Calculate state's share of electoral votes for each county\n",
    "    df_with_ev['state_total_votes'] = df_with_ev.groupby('state_name')['avg_total_votes'].transform('sum')\n",
    "    df_with_ev['county_vote_share'] = df_with_ev['avg_total_votes'] / df_with_ev['state_total_votes']\n",
    "    df_with_ev['county_electoral_importance'] = df_with_ev['county_vote_share'] * df_with_ev['electoral_votes']\n",
    "    \n",
    "    # Add columns for electoral importance considering competitiveness\n",
    "    df_with_ev['competitive_electoral_importance'] = (\n",
    "        df_with_ev['county_electoral_importance'] * \n",
    "        (1 - abs(df_with_ev['avg_margin'])/100)  # Counties with closer margins get higher weight\n",
    "    )\n",
    "    \n",
    "    return df_with_ev\n",
    "\n",
    "def analyze_state_importance(df_with_ev):\n",
    "    \"\"\"\n",
    "    Analyzes state-level electoral importance based on county data.\n",
    "    \n",
    "    Parameters:\n",
    "    df_with_ev (pandas.DataFrame): DataFrame with electoral vote data added\n",
    "    \n",
    "    Returns:\n",
    "    dict: Different metrics of state importance\n",
    "    \"\"\"\n",
    "    state_analysis = {}\n",
    "    \n",
    "    # Aggregate to state level\n",
    "    state_summary = df_with_ev.groupby('state_name').agg({\n",
    "        'electoral_votes': 'first',  # Each state has same value for all counties\n",
    "        'avg_margin': 'mean',\n",
    "        'margin_volatility': 'mean',\n",
    "        'avg_total_votes': 'sum',\n",
    "        'party_changes': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate state importance metrics\n",
    "    state_summary['state_power_index'] = (\n",
    "        (state_summary['electoral_votes'] / state_summary['electoral_votes'].max()) *  # Normalized electoral votes\n",
    "        (1 - abs(state_summary['avg_margin'])/100) *  # Competitiveness factor\n",
    "        (1 + state_summary['margin_volatility'])  # Volatility bonus\n",
    "    )\n",
    "    \n",
    "    # Get different rankings\n",
    "    state_analysis['most_powerful_states'] = state_summary.nlargest(10, 'state_power_index')\n",
    "    state_analysis['most_competitive_states'] = state_summary.nlargest(10, 'margin_volatility')\n",
    "    state_analysis['largest_electoral_votes'] = state_summary.nlargest(10, 'electoral_votes')\n",
    "    \n",
    "    return state_analysis\n",
    "\n",
    "def visualize_state_importance(df_with_ev):\n",
    "    \"\"\"\n",
    "    Creates visualizations of state electoral importance.\n",
    "    \n",
    "    Parameters:\n",
    "    df_with_ev (pandas.DataFrame): DataFrame with electoral vote data added\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: Figure containing visualizations\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # State Power Index Plot\n",
    "    state_summary = df_with_ev.groupby('state_name').agg({\n",
    "        'electoral_votes': 'first',\n",
    "        'avg_margin': 'mean',\n",
    "        'margin_volatility': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(state_summary['avg_margin'], \n",
    "               state_summary['electoral_votes'],\n",
    "               s=state_summary['margin_volatility']*100,\n",
    "               alpha=0.6)\n",
    "    \n",
    "    # Label important states\n",
    "    for idx, row in state_summary.nlargest(10, 'electoral_votes').iterrows():\n",
    "        plt.annotate(row['state_name'], \n",
    "                    (row['avg_margin'], row['electoral_votes']))\n",
    "    \n",
    "    plt.xlabel('Average Margin (%)')\n",
    "    plt.ylabel('Electoral Votes')\n",
    "    plt.title('State Electoral Importance\\n(Bubble size = Margin Volatility)')\n",
    "    \n",
    "    # Electoral Votes Distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    state_summary.nlargest(15, 'electoral_votes').plot(\n",
    "        kind='bar', \n",
    "        x='state_name', \n",
    "        y='electoral_votes',\n",
    "        title='Top 15 States by Electoral Votes'\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis and get visualizations\n",
    "results, fig = analyze_and_visualize_elections(df_race_and_election)\n",
    "\n",
    "# Display the visualizations\n",
    "plt.show()\n",
    "\n",
    "# View the results\n",
    "for category, data in results.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census_election_data move state_name, county_name to the first columns\n",
    "def move_columns_to_front(df, columns):\n",
    "    \"\"\"\n",
    "    Moves specified columns to the front of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to modify\n",
    "    columns (list): List of column names to move to the front\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with specified columns moved to the front\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get list of columns not in the specified list\n",
    "    other_columns = [col for col in df.columns if col not in columns]\n",
    "    \n",
    "    # Reorder columns\n",
    "    new_columns = columns + other_columns\n",
    "    df = df[new_columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "census_election_data = move_columns_to_front(census_election_data, ['state_name', 'county_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vote Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "import folium\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def analyze_spatial_vote_flipping(df):\n",
    "    \"\"\"\n",
    "    Analyzes spatial correlation in county voting pattern changes.\n",
    "    \n",
    "    Parameters:\n",
    "    df (GeoDataFrame): DataFrame containing county election data and geometries\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (analysis_results, figures) containing statistics and visualizations\n",
    "    \"\"\"\n",
    "    # Convert to GeoDataFrame if not already\n",
    "    gdf = gpd.GeoDataFrame(df) if not isinstance(df, gpd.GeoDataFrame) else df\n",
    "    \n",
    "    # Calculate flips between elections\n",
    "    gdf['flip_2012_2016'] = (gdf['winner_2012'] != gdf['winner_2016']).astype(int)\n",
    "    gdf['flip_2016_2020'] = (gdf['winner_2016'] != gdf['winner_2020']).astype(int)\n",
    "    gdf['total_flips'] = gdf['flip_2012_2016'] + gdf['flip_2016_2020']\n",
    "    \n",
    "    # Create spatial weights matrix using Queen contiguity\n",
    "    weights = Queen.from_dataframe(gdf)\n",
    "    weights.transform = 'r'  # Row-standardize weights\n",
    "    \n",
    "    # Calculate Moran's I for different periods\n",
    "    moran_2012_2016 = Moran(gdf['flip_2012_2016'], weights)\n",
    "    moran_2016_2020 = Moran(gdf['flip_2016_2020'], weights)\n",
    "    moran_total = Moran(gdf['total_flips'], weights)\n",
    "    \n",
    "    # Calculate local Moran's I for clustering analysis\n",
    "    from esda.moran import Moran_Local\n",
    "    local_moran = Moran_Local(gdf['total_flips'], weights)\n",
    "    \n",
    "    # Add cluster categories to the dataframe\n",
    "    gdf['cluster_category'] = 'Not Significant'\n",
    "    # High-High\n",
    "    gdf.loc[(local_moran.p_sim < 0.05) & (gdf['total_flips'] > gdf['total_flips'].mean()) & \n",
    "            (local_moran.q == 1), 'cluster_category'] = 'High-High'\n",
    "    # Low-Low\n",
    "    gdf.loc[(local_moran.p_sim < 0.05) & (gdf['total_flips'] < gdf['total_flips'].mean()) & \n",
    "            (local_moran.q == 2), 'cluster_category'] = 'Low-Low'\n",
    "    # High-Low\n",
    "    gdf.loc[(local_moran.p_sim < 0.05) & (gdf['total_flips'] > gdf['total_flips'].mean()) & \n",
    "            (local_moran.q == 3), 'cluster_category'] = 'High-Low'\n",
    "    # Low-High\n",
    "    gdf.loc[(local_moran.p_sim < 0.05) & (gdf['total_flips'] < gdf['total_flips'].mean()) & \n",
    "            (local_moran.q == 4), 'cluster_category'] = 'Low-High'\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    # Plot 1: 2012-2016 flips\n",
    "    gdf.plot(column='flip_2012_2016', \n",
    "            ax=axes[0,0],\n",
    "            legend=True,\n",
    "            legend_kwds={'label': 'County Flipped 2012-2016'},\n",
    "            cmap='RdBu')\n",
    "    axes[0,0].set_title(\"County Flips 2012-2016\")\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # Plot 2: 2016-2020 flips\n",
    "    gdf.plot(column='flip_2016_2020',\n",
    "            ax=axes[0,1],\n",
    "            legend=True,\n",
    "            legend_kwds={'label': 'County Flipped 2016-2020'},\n",
    "            cmap='RdBu')\n",
    "    axes[0,1].set_title(\"County Flips 2016-2020\")\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    # Plot 3: Total flips\n",
    "    gdf.plot(column='total_flips',\n",
    "            ax=axes[1,0],\n",
    "            legend=True,\n",
    "            legend_kwds={'label': 'Total Flips'},\n",
    "            cmap='viridis')\n",
    "    axes[1,0].set_title(\"Total County Flips 2012-2020\")\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # Plot 4: Cluster categories\n",
    "    gdf.plot(column='cluster_category',\n",
    "            ax=axes[1,1],\n",
    "            legend=True,\n",
    "            legend_kwds={'label': 'Spatial Clusters'},\n",
    "            categorical=True,\n",
    "            cmap='Set3')\n",
    "    axes[1,1].set_title(\"Spatial Clustering of Flips\")\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'global_spatial_correlation': {\n",
    "            '2012-2016': {\n",
    "                'morans_i': moran_2012_2016.I,\n",
    "                'p_value': moran_2012_2016.p_sim\n",
    "            },\n",
    "            '2016-2020': {\n",
    "                'morans_i': moran_2016_2020.I,\n",
    "                'p_value': moran_2016_2020.p_sim\n",
    "            },\n",
    "            'total': {\n",
    "                'morans_i': moran_total.I,\n",
    "                'p_value': moran_total.p_sim\n",
    "            }\n",
    "        },\n",
    "        'cluster_summary': gdf['cluster_category'].value_counts().to_dict(),\n",
    "        'flip_summary': {\n",
    "            'total_flipping_counties_2012_2016': gdf['flip_2012_2016'].sum(),\n",
    "            'total_flipping_counties_2016_2020': gdf['flip_2016_2020'].sum(),\n",
    "            'counties_that_flipped_twice': len(gdf[gdf['total_flips'] == 2])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, fig, gdf\n",
    "\n",
    "def print_spatial_analysis_results(results):\n",
    "    \"\"\"\n",
    "    Prints formatted analysis results\n",
    "    \"\"\"\n",
    "    print(\"Spatial Correlation Analysis of County Vote Flipping\\n\")\n",
    "    \n",
    "    print(\"Global Moran's I Statistics:\")\n",
    "    for period, stats in results['global_spatial_correlation'].items():\n",
    "        print(f\"\\n{period}:\")\n",
    "        print(f\"Moran's I: {stats['morans_i']:.3f}\")\n",
    "        print(f\"P-value: {stats['p_value']:.3f}\")\n",
    "    \n",
    "    print(\"\\nCluster Analysis:\")\n",
    "    for cluster_type, count in results['cluster_summary'].items():\n",
    "        print(f\"{cluster_type}: {count} counties\")\n",
    "    \n",
    "    print(\"\\nFlip Summary:\")\n",
    "    for metric, value in results['flip_summary'].items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "results, fig, gdf_with_clusters = analyze_spatial_vote_flipping(df_race_and_election)\n",
    "\n",
    "# Print the results\n",
    "print_spatial_analysis_results(results)\n",
    "\n",
    "# Show the maps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### County election importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_county_importance(gdf, state_electoral_votes=None):\n",
    "    \"\"\"\n",
    "    Analyzes county importance in presidential elections based on multiple factors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        GeoDataFrame with county election data and geometries\n",
    "    state_electoral_votes : dict, optional\n",
    "        Dictionary of state electoral votes. If None, uses 2020 allocation.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with importance metrics, Figure with visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Electoral votes by state (2020/2024 allocation)\n",
    "    if state_electoral_votes is None:\n",
    "        state_electoral_votes = {\n",
    "            'Alabama': 9, 'Alaska': 3, 'Arizona': 11, 'Arkansas': 6, 'California': 54,\n",
    "            'Colorado': 10, 'Connecticut': 7, 'Delaware': 3, 'District of Columbia': 3,\n",
    "            'Florida': 30, 'Georgia': 16, 'Hawaii': 4, 'Idaho': 4, 'Illinois': 19,\n",
    "            'Indiana': 11, 'Iowa': 6, 'Kansas': 6, 'Kentucky': 8, 'Louisiana': 8,\n",
    "            'Maine': 4, 'Maryland': 10, 'Massachusetts': 11, 'Michigan': 15,\n",
    "            'Minnesota': 10, 'Mississippi': 6, 'Missouri': 10, 'Montana': 4,\n",
    "            'Nebraska': 5, 'Nevada': 6, 'New Hampshire': 4, 'New Jersey': 14,\n",
    "            'New Mexico': 5, 'New York': 28, 'North Carolina': 16, 'North Dakota': 3,\n",
    "            'Ohio': 17, 'Oklahoma': 7, 'Oregon': 8, 'Pennsylvania': 19,\n",
    "            'Rhode Island': 4, 'South Carolina': 9, 'South Dakota': 3,\n",
    "            'Tennessee': 11, 'Texas': 40, 'Utah': 6, 'Vermont': 3, 'Virginia': 13,\n",
    "            'Washington': 12, 'West Virginia': 4, 'Wisconsin': 10, 'Wyoming': 3\n",
    "        }\n",
    "    \n",
    "    # Create working copy\n",
    "    gdf = gdf.copy()\n",
    "    \n",
    "    # Add electoral votes\n",
    "    gdf['electoral_votes'] = gdf['state_name'].map(state_electoral_votes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    \n",
    "    # 1. Average margin across elections (closer = more important)\n",
    "    gdf['avg_margin'] = np.abs(pd.concat([\n",
    "        gdf['per_point_diff_2012'],\n",
    "        gdf['per_point_diff_2016'],\n",
    "        gdf['per_point_diff_2020']\n",
    "    ], axis=1)).mean(axis=1)\n",
    "    \n",
    "    # 2. Margin volatility (more volatile = more important)\n",
    "    gdf['margin_volatility'] = np.abs(\n",
    "        gdf['per_point_diff_2016'] - gdf['per_point_diff_2012']\n",
    "    ) + np.abs(\n",
    "        gdf['per_point_diff_2020'] - gdf['per_point_diff_2016']\n",
    "    )\n",
    "    \n",
    "    # 3. Average total votes (more votes = more important)\n",
    "    gdf['avg_total_votes'] = pd.concat([\n",
    "        gdf['total_votes_2012'],\n",
    "        gdf['total_votes_2016'],\n",
    "        gdf['total_votes_2020']\n",
    "    ], axis=1).mean(axis=1)\n",
    "    \n",
    "    # 4. County's share of state's electoral votes\n",
    "    gdf['state_total_votes'] = gdf.groupby('state_name')['avg_total_votes'].transform('sum')\n",
    "    gdf['electoral_vote_share'] = (\n",
    "        gdf['avg_total_votes'] / gdf['state_total_votes'] * gdf['electoral_votes']\n",
    "    )\n",
    "    \n",
    "    # 5. Calculate swing state bonus\n",
    "    gdf['swing_state_bonus'] = (\n",
    "        (1 - np.abs(gdf['per_point_diff_2020'] / 100)) * \n",
    "        (gdf['margin_volatility'] / gdf['margin_volatility'].max())\n",
    "    )\n",
    "    \n",
    "    # Calculate final importance score\n",
    "    # Normalize components\n",
    "    for col in ['avg_margin', 'margin_volatility', 'avg_total_votes', 'electoral_vote_share']:\n",
    "        gdf[f'{col}_norm'] = (gdf[col] - gdf[col].min()) / (gdf[col].max() - gdf[col].min())\n",
    "    \n",
    "    gdf['importance_score'] = (\n",
    "        (1 - gdf['avg_margin_norm']) * 0.3 +  # Close margins (30% weight)\n",
    "        gdf['margin_volatility_norm'] * 0.2 +  # Volatility (20% weight)\n",
    "        gdf['avg_total_votes_norm'] * 0.2 +    # Vote volume (20% weight)\n",
    "        gdf['electoral_vote_share'] * 0.2 +    # Electoral importance (20% weight)\n",
    "        gdf['swing_state_bonus'] * 0.1         # Swing state bonus (10% weight)\n",
    "    )\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    # Plot 1: Overall Importance Score\n",
    "    gdf.plot(\n",
    "        column='importance_score',\n",
    "        ax=axes[0,0],\n",
    "        legend=True,\n",
    "        cmap='viridis',\n",
    "        legend_kwds={'label': 'County Importance Score'}\n",
    "    )\n",
    "    axes[0,0].set_title('County Electoral Importance')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # Plot 2: Margin Volatility\n",
    "    gdf.plot(\n",
    "        column='margin_volatility',\n",
    "        ax=axes[0,1],\n",
    "        legend=True,\n",
    "        cmap='RdYlBu',\n",
    "        legend_kwds={'label': 'Margin Volatility'}\n",
    "    )\n",
    "    axes[0,1].set_title('County Vote Margin Volatility')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    # Plot 3: Electoral Vote Share\n",
    "    gdf.plot(\n",
    "        column='electoral_vote_share',\n",
    "        ax=axes[1,0],\n",
    "        legend=True,\n",
    "        cmap='Reds',\n",
    "        legend_kwds={'label': 'Electoral Vote Share'}\n",
    "    )\n",
    "    axes[1,0].set_title('County Share of Electoral Votes')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # Plot 4: Swing State Bonus\n",
    "    gdf.plot(\n",
    "        column='swing_state_bonus',\n",
    "        ax=axes[1,1],\n",
    "        legend=True,\n",
    "        cmap='coolwarm',\n",
    "        legend_kwds={'label': 'Swing State Bonus'}\n",
    "    )\n",
    "    axes[1,1].set_title('Swing State Importance')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Get top counties\n",
    "    top_counties = gdf.nlargest(20, 'importance_score')[\n",
    "        ['county_name', 'state_name', 'importance_score', \n",
    "         'avg_margin', 'margin_volatility', 'electoral_vote_share']\n",
    "    ]\n",
    "    \n",
    "    return gdf, fig, top_counties\n",
    "\n",
    "def print_importance_analysis(top_counties):\n",
    "    \"\"\"\n",
    "    Prints formatted analysis of most important counties\n",
    "    \"\"\"\n",
    "    print(\"\\nMost Important Counties for Electoral Victory:\\n\")\n",
    "    \n",
    "    for idx, county in top_counties.iterrows():\n",
    "        print(f\"{county['county_name']}, {county['state_name']}\")\n",
    "        print(f\"  Importance Score: {county['importance_score']:.3f}\")\n",
    "        print(f\"  Average Margin: {county['avg_margin']:.1f}%\")\n",
    "        print(f\"  Margin Volatility: {county['margin_volatility']:.1f}\")\n",
    "        print(f\"  Electoral Vote Share: {county['electoral_vote_share']:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "gdf_with_importance, fig, top_counties = analyze_county_importance(df_race_and_election)\n",
    "\n",
    "# Print the results\n",
    "print_importance_analysis(top_counties)\n",
    "\n",
    "# Show the maps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gentriification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_gentrification(df):\n",
    "    \"\"\"\n",
    "    Analyzes county-level gentrification using multiple indicators.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame containing county-level census and demographic data\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with gentrification scores and component metrics\n",
    "    \"\"\"\n",
    "    # Create working copy\n",
    "    gdf = df.copy()\n",
    "    \n",
    "    # 1. Educational Change (2012-2020)\n",
    "    gdf['edu_change_score'] = (\n",
    "        (gdf[\"Percent of adults with a bachelor's degree or higher, 2018-22\"] -\n",
    "         gdf[\"Percent of adults with a bachelor's degree or higher, 2008-12\"]) / \n",
    "        gdf[\"Percent of adults with a bachelor's degree or higher, 2008-12\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Income Changes (using median household income)\n",
    "    gdf['income_change_score'] = (\n",
    "        (gdf['Median_Household_Income_2021'] -\n",
    "         gdf['median_household_income_dollars_occupied_housing_units_2012']) /\n",
    "        gdf['median_household_income_dollars_occupied_housing_units_2012']\n",
    "    )\n",
    "    \n",
    "    # 3. Demographic Changes (2012-2020)\n",
    "    gdf['demographic_change_score'] = (\n",
    "        # Change in white population percentage\n",
    "        np.abs(gdf['pct_White alone_2020'] - gdf['pct_White alone_2012']) +\n",
    "        # Change in Black population percentage\n",
    "        np.abs(gdf['pct_Black or African American alone_2020'] - gdf['pct_Black or African American alone_2012']) +\n",
    "        # Change in Asian population percentage\n",
    "        np.abs(gdf['pct_Asian alone_2020'] - gdf['pct_Asian alone_2012'])\n",
    "    )\n",
    "    \n",
    "    # 4. Housing Cost Changes\n",
    "    # Calculate change in housing costs using rent data\n",
    "    gdf['housing_cost_change_score'] = (\n",
    "        (gdf['median_dollars_occupied_housing_units_2016'] -  # Using 2016 as endpoint due to data availability\n",
    "         gdf['median_dollars_occupied_housing_units_2012']) /\n",
    "        gdf['median_dollars_occupied_housing_units_2012']\n",
    "    )\n",
    "    \n",
    "    # 5. Employment Changes\n",
    "    gdf['employment_change_score'] = (\n",
    "        (gdf['Unemployment_rate_2022'] - gdf['Unemployment_rate_2012'])\n",
    "    )\n",
    "    \n",
    "    # Calculate overall gentrification score\n",
    "    # Normalize each component\n",
    "    for col in ['edu_change_score', 'income_change_score', 'demographic_change_score', \n",
    "                'housing_cost_change_score', 'employment_change_score']:\n",
    "        gdf[f'{col}_norm'] = (gdf[col] - gdf[col].min()) / (gdf[col].max() - gdf[col].min())\n",
    "    \n",
    "    # Weighted combination for final score\n",
    "    gdf['gentrification_score'] = (\n",
    "        gdf['edu_change_score_norm'] * 0.25 +              # Education change\n",
    "        gdf['income_change_score_norm'] * 0.25 +           # Income change\n",
    "        gdf['demographic_change_score_norm'] * 0.2 +       # Demographic change\n",
    "        gdf['housing_cost_change_score_norm'] * 0.2 +      # Housing cost change\n",
    "        gdf['employment_change_score_norm'] * 0.1          # Employment change\n",
    "    )\n",
    "    \n",
    "    # Categorize gentrification levels\n",
    "    gdf['gentrification_level'] = pd.qcut(\n",
    "        gdf['gentrification_score'], \n",
    "        q=5, \n",
    "        labels=['Very Low', 'Low', 'Moderate', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def visualize_gentrification(gdf):\n",
    "    \"\"\"\n",
    "    Creates visualizations of gentrification analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf: GeoDataFrame with gentrification metrics\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    # Plot 1: Overall Gentrification Score\n",
    "    gdf.plot(\n",
    "        column='gentrification_score',\n",
    "        ax=axes[0,0],\n",
    "        legend=True,\n",
    "        cmap='RdYlBu_r',\n",
    "        legend_kwds={'label': 'Gentrification Score'}\n",
    "    )\n",
    "    axes[0,0].set_title('Overall Gentrification Score')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # Plot 2: Education and Income Changes\n",
    "    ax2 = axes[0,1].scatter(\n",
    "        gdf['edu_change_score'],\n",
    "        gdf['income_change_score'],\n",
    "        c=gdf['gentrification_score'],\n",
    "        cmap='RdYlBu_r',\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[0,1].set_xlabel('Educational Attainment Change')\n",
    "    axes[0,1].set_ylabel('Income Change')\n",
    "    axes[0,1].set_title('Education vs Income Changes')\n",
    "    plt.colorbar(ax2, ax=axes[0,1], label='Gentrification Score')\n",
    "    \n",
    "    # Plot 3: Housing Cost Change\n",
    "    gdf.plot(\n",
    "        column='housing_cost_change_score',\n",
    "        ax=axes[1,0],\n",
    "        legend=True,\n",
    "        cmap='Reds',\n",
    "        legend_kwds={'label': 'Housing Cost Change'}\n",
    "    )\n",
    "    axes[1,0].set_title('Housing Cost Changes')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # Plot 4: Demographic Change\n",
    "    gdf.plot(\n",
    "        column='demographic_change_score',\n",
    "        ax=axes[1,1],\n",
    "        legend=True,\n",
    "        cmap='viridis',\n",
    "        legend_kwds={'label': 'Demographic Change Score'}\n",
    "    )\n",
    "    axes[1,1].set_title('Demographic Changes')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def print_gentrification_summary(gdf):\n",
    "    \"\"\"\n",
    "    Prints summary statistics and identifies most gentrified counties.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf: GeoDataFrame with gentrification metrics\n",
    "    \"\"\"\n",
    "    print(\"Gentrification Analysis Summary\\n\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"Overall Statistics:\")\n",
    "    print(f\"Mean Gentrification Score: {gdf['gentrification_score'].mean():.3f}\")\n",
    "    print(f\"Median Gentrification Score: {gdf['gentrification_score'].median():.3f}\")\n",
    "    \n",
    "    print(\"\\nDistribution by Gentrification Level:\")\n",
    "    print(gdf['gentrification_level'].value_counts())\n",
    "    \n",
    "    print(\"\\nTop 10 Most Gentrified Counties:\")\n",
    "    top_10 = gdf.nlargest(10, 'gentrification_score')[\n",
    "        ['county_name', 'state_name', 'gentrification_score', \n",
    "         'edu_change_score', 'income_change_score', 'housing_cost_change_score']\n",
    "    ]\n",
    "    for idx, row in top_10.iterrows():\n",
    "        print(f\"\\n{row['county_name']}, {row['state_name']}\")\n",
    "        print(f\"  Gentrification Score: {row['gentrification_score']:.3f}\")\n",
    "        print(f\"  Education Change: {row['edu_change_score']:.1%}\")\n",
    "        print(f\"  Income Change: {row['income_change_score']:.1%}\")\n",
    "        print(f\"  Housing Cost Change: {row['housing_cost_change_score']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "gdf_with_gentrification = analyze_gentrification(df2)\n",
    "\n",
    "# Create visualizations\n",
    "fig = visualize_gentrification(gdf_with_gentrification)\n",
    "\n",
    "# Print summary statistics\n",
    "print_gentrification_summary(gdf_with_gentrification)\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_industry_occupation_changes(df):\n",
    "    \"\"\"\n",
    "    Analyzes changes in occupational composition within industries across 2012, 2016, and 2020.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame containing industry and occupation data\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of (changes_df, summary_stats)\n",
    "    \"\"\"\n",
    "    # Define main industries (excluding total employment)\n",
    "    industries = {\n",
    "        'agriculture': 'agriculture_forestry_fishing_and_hunting_and_mining',\n",
    "        'arts': 'arts_entertainment_and_recreation_and_accommodation_and_food_services',\n",
    "        'construction': 'construction',\n",
    "        'education': 'educational_services_and_health_care_and_social_assistance',\n",
    "        'finance': 'finance_and_insurance_and_real_estate_and_rental_and_leasing',\n",
    "        'information': 'information',\n",
    "        'manufacturing': 'manufacturing',\n",
    "        'other_services': 'other_services_except_public_administration',\n",
    "        'professional': 'professional_scientific_and_management_and_administrative_and_waste_management_services',\n",
    "        'public_admin': 'public_administration',\n",
    "        'retail': 'retail_trade',\n",
    "        'transportation': 'transportation_and_warehousing_and_utilities',\n",
    "        'wholesale': 'wholesale_trade'\n",
    "    }\n",
    "    \n",
    "    # Define occupations\n",
    "    occupations = {\n",
    "        'management': 'management_business_science_and_arts_occupations',\n",
    "        'service': 'service_occupations',\n",
    "        'sales': 'sales_and_office_occupations',\n",
    "        'construction': 'natural_resources_construction_and_maintenance_occupations',\n",
    "        'production': 'production_transportation_and_material_moving_occupations'\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionaries to store DataFrames for concatenation\n",
    "    base_values = {}\n",
    "    changes_12_16 = {}\n",
    "    changes_16_20 = {}\n",
    "    changes_12_20 = {}\n",
    "    pct_changes_12_16 = {}\n",
    "    pct_changes_16_20 = {}\n",
    "    pct_changes_12_20 = {}\n",
    "    \n",
    "    # Calculate all values and changes\n",
    "    for ind_key, industry in industries.items():\n",
    "        for occ_key, occupation in occupations.items():\n",
    "            # Column pattern for this industry-occupation combination\n",
    "            col_pattern = f\"INDUSTRY_{industry}_OCCUPATION_{occupation}_percent\"\n",
    "            base_name = f'{ind_key}_{occ_key}'\n",
    "            \n",
    "            # Get values for each year\n",
    "            values = {}\n",
    "            for year in [2012, 2016, 2020]:\n",
    "                col_name = f\"{col_pattern}_{year}\"\n",
    "                if col_name in df.columns:\n",
    "                    values[year] = df[col_name]\n",
    "                    base_values[f'{base_name}_{year}'] = values[year]\n",
    "            \n",
    "            # Calculate changes if we have all years\n",
    "            if len(values) == 3:\n",
    "                changes_12_16[base_name] = values[2016] - values[2012]\n",
    "                changes_16_20[base_name] = values[2020] - values[2016]\n",
    "                changes_12_20[base_name] = values[2020] - values[2012]\n",
    "                \n",
    "                # Calculate percentage changes\n",
    "                pct_changes_12_16[base_name] = ((values[2016] - values[2012]) / values[2012] * 100).replace([np.inf, -np.inf], np.nan)\n",
    "                pct_changes_16_20[base_name] = ((values[2020] - values[2016]) / values[2016] * 100).replace([np.inf, -np.inf], np.nan)\n",
    "                pct_changes_12_20[base_name] = ((values[2020] - values[2012]) / values[2012] * 100).replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    changes = pd.DataFrame({\n",
    "        'county_name': df['county_name'],\n",
    "        'state_name': df['state_name']\n",
    "    })\n",
    "    \n",
    "    # Concatenate all the calculated values\n",
    "    if base_values:\n",
    "        changes = pd.concat([changes, pd.DataFrame(base_values)], axis=1)\n",
    "    \n",
    "    # Add the changes columns with appropriate suffixes\n",
    "    for base_name in changes_12_16.keys():\n",
    "        changes = pd.concat([\n",
    "            changes,\n",
    "            pd.DataFrame({\n",
    "                f'{base_name}_change_12_16': changes_12_16[base_name],\n",
    "                f'{base_name}_change_16_20': changes_16_20[base_name],\n",
    "                f'{base_name}_change_12_20': changes_12_20[base_name],\n",
    "                f'{base_name}_pct_change_12_16': pct_changes_12_16[base_name],\n",
    "                f'{base_name}_pct_change_16_20': pct_changes_16_20[base_name],\n",
    "                f'{base_name}_pct_change_12_20': pct_changes_12_20[base_name]\n",
    "            })\n",
    "        ], axis=1)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary_stats = {\n",
    "        'industry_changes': {},\n",
    "        'top_shifts': {},\n",
    "        'volatility': {}\n",
    "    }\n",
    "    \n",
    "    for ind_key, industry in industries.items():\n",
    "        industry_stats = {}\n",
    "        for occ_key, occupation in occupations.items():\n",
    "            base_name = f'{ind_key}_{occ_key}'\n",
    "            \n",
    "            if f'{base_name}_change_12_20' in changes.columns:\n",
    "                # Overall changes\n",
    "                industry_stats[occ_key] = {\n",
    "                    '2012-2016': changes[f'{base_name}_change_12_16'].mean(),\n",
    "                    '2016-2020': changes[f'{base_name}_change_16_20'].mean(),\n",
    "                    '2012-2020': changes[f'{base_name}_change_12_20'].mean()\n",
    "                }\n",
    "                \n",
    "                # Calculate volatility\n",
    "                summary_stats['volatility'][base_name] = changes[[\n",
    "                    f'{base_name}_change_12_16',\n",
    "                    f'{base_name}_change_16_20'\n",
    "                ]].std().mean()\n",
    "                \n",
    "                # Top shifts\n",
    "                summary_stats['top_shifts'][base_name] = {\n",
    "                    'increasing': changes.nlargest(\n",
    "                        5, f'{base_name}_change_12_20'\n",
    "                    )[['county_name', 'state_name', f'{base_name}_change_12_20']],\n",
    "                    'decreasing': changes.nsmallest(\n",
    "                        5, f'{base_name}_change_12_20'\n",
    "                    )[['county_name', 'state_name', f'{base_name}_change_12_20']]\n",
    "                }\n",
    "        \n",
    "        summary_stats['industry_changes'][ind_key] = industry_stats\n",
    "    \n",
    "    return changes, summary_stats\n",
    "\n",
    "def print_industry_summary(summary_stats, industry_name):\n",
    "    \"\"\"\n",
    "    Prints formatted summary of occupational changes within an industry.\n",
    "    \n",
    "    Parameters:\n",
    "    summary_stats: Dictionary containing summary statistics\n",
    "    industry_name: Key of the industry to summarize\n",
    "    \"\"\"\n",
    "    print(f\"\\nOccupational Changes in {industry_name.title()} Industry\\n\")\n",
    "    \n",
    "    if industry_name in summary_stats['industry_changes']:\n",
    "        industry_stats = summary_stats['industry_changes'][industry_name]\n",
    "        \n",
    "        print(\"Overall Changes (Percentage Points):\")\n",
    "        for occupation, changes in industry_stats.items():\n",
    "            print(f\"\\n{occupation.title()}:\")\n",
    "            print(f\"  2012-2016: {changes['2012-2016']:.3f}\")\n",
    "            print(f\"  2016-2020: {changes['2016-2020']:.3f}\")\n",
    "            print(f\"  2012-2020: {changes['2012-2020']:.3f}\")\n",
    "        \n",
    "        print(\"\\nMost Volatile Occupation Combinations:\")\n",
    "        relevant_volatility = {k: v for k, v in summary_stats['volatility'].items() \n",
    "                             if k.startswith(industry_name)}\n",
    "        sorted_volatility = sorted(\n",
    "            relevant_volatility.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        for combo, volatility in sorted_volatility:\n",
    "            print(f\"{combo.replace(f'{industry_name}_', '')}: {volatility:.3f}\")\n",
    "\n",
    "def visualize_industry_occupation_changes(changes_df, industry_name):\n",
    "    \"\"\"\n",
    "    Creates visualizations of occupational changes within a specific industry.\n",
    "    \n",
    "    Parameters:\n",
    "    changes_df: DataFrame containing calculated changes\n",
    "    industry_name: Key of the industry to visualize\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib figure\n",
    "    \"\"\"\n",
    "    occupations = ['management', 'service', 'sales', 'construction', 'production']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot 1: Box plot of percentage changes with outlier handling\n",
    "    plt.subplot(2, 2, 1)\n",
    "    change_data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Define reasonable bounds for percentage changes\n",
    "    lower_bound = -100  # Can't decrease more than 100%\n",
    "    upper_bound = 200   # Limit increases to 200%\n",
    "    \n",
    "    for occ in occupations:\n",
    "        base_name = f'{industry_name}_{occ}'\n",
    "        if f'{base_name}_pct_change_12_16' in changes_df.columns:\n",
    "            # Clip the percentage changes to reasonable bounds\n",
    "            data_12_16 = changes_df[f'{base_name}_pct_change_12_16'].clip(lower_bound, upper_bound).dropna()\n",
    "            data_16_20 = changes_df[f'{base_name}_pct_change_16_20'].clip(lower_bound, upper_bound).dropna()\n",
    "            \n",
    "            change_data.extend([data_12_16, data_16_20])\n",
    "            labels.extend([f'{occ}\\n2012-2016', f'{occ}\\n2016-2020'])\n",
    "    \n",
    "    plt.boxplot(change_data, labels=labels, whis=1.5)  # Use 1.5 IQR for whiskers\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Distribution of Percentage Changes by Occupation\\nin {industry_name.title()} Industry')\n",
    "    plt.ylabel('Percentage Change')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.2)  # Add reference line at 0\n",
    "    \n",
    "    # Add note about bounds\n",
    "    plt.text(0.02, 0.98, f'Note: Changes clipped to [{lower_bound}%, {upper_bound}%]', \n",
    "             transform=plt.gca().transAxes, fontsize=8, verticalalignment='top')\n",
    "    \n",
    "    # Plot 2: Average composition over time\n",
    "    plt.subplot(2, 2, 2)\n",
    "    years = [2012, 2016, 2020]\n",
    "    for occ in occupations:\n",
    "        base_name = f'{industry_name}_{occ}'\n",
    "        if all(f'{base_name}_{year}' in changes_df.columns for year in years):\n",
    "            values = [\n",
    "                changes_df[f'{base_name}_{year}'].mean() for year in years\n",
    "            ]\n",
    "            plt.plot(years, values, marker='o', label=occ)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'Average Occupation Percentages Over Time\\nin {industry_name.title()} Industry')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Heatmap of correlations between occupation changes\n",
    "    plt.subplot(2, 2, 3)\n",
    "    \n",
    "    # Use absolute changes instead of percentage changes for correlation\n",
    "    change_cols = [f'{industry_name}_{occ}_change_12_20' for occ in occupations \n",
    "                  if f'{industry_name}_{occ}_change_12_20' in changes_df.columns]\n",
    "    if change_cols:\n",
    "        correlation_matrix = changes_df[change_cols].corr()\n",
    "        \n",
    "        sns.heatmap(\n",
    "            correlation_matrix,\n",
    "            annot=True,\n",
    "            cmap='RdBu',\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt='.2f'\n",
    "        )\n",
    "        plt.title(f'Correlation of Changes Between Occupations\\nin {industry_name.title()} Industry')\n",
    "        \n",
    "        # Clean up axis labels\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        labels = [col.replace(f'{industry_name}_', '').replace('_change_12_20', '') for col in change_cols]\n",
    "        plt.xticks(np.arange(len(labels)) + 0.5, labels)\n",
    "        plt.yticks(np.arange(len(labels)) + 0.5, labels)\n",
    "    \n",
    "    # Add plot 4: Distribution of raw percentages in 2020\n",
    "    plt.subplot(2, 2, 4)\n",
    "    current_data = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for occ in occupations:\n",
    "        base_name = f'{industry_name}_{occ}'\n",
    "        if f'{base_name}_2020' in changes_df.columns:\n",
    "            current_data.append(changes_df[f'{base_name}_2020'].dropna())\n",
    "            current_labels.append(occ)\n",
    "    \n",
    "    if current_data:\n",
    "        plt.boxplot(current_data, labels=current_labels)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Distribution of Occupation Percentages (2020)\\nin {industry_name.title()} Industry')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "changes, summary = analyze_industry_occupation_changes(census_election_data)\n",
    "\n",
    "# Analyze a specific industry (e.g., manufacturing)\n",
    "fig = visualize_industry_occupation_changes(changes, 'manufacturing')\n",
    "print_industry_summary(summary, 'manufacturing')\n",
    "\n",
    "# You can analyze any industry using its key:\n",
    "# 'agriculture', 'arts', 'construction', 'education', 'finance', \n",
    "# 'information', 'manufacturing', 'other_services', 'professional', \n",
    "# 'public_admin', 'retail', 'transportation', 'wholesale'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = [\n",
    "    'geometry', 'county_name', 'state_name',\n",
    "    \n",
    "    # Election data 2012\n",
    "    'total_votes_2012', 'votes_dem_2012', 'votes_gop_2012',\n",
    "    'per_dem_2012', 'per_gop_2012', 'diff_2012', 'per_point_diff_2012',\n",
    "    \n",
    "    # Election data 2016\n",
    "    'total_votes_2016', 'votes_dem_2016', 'votes_gop_2016',\n",
    "    'per_dem_2016', 'per_gop_2016', 'diff_2016', 'per_point_diff_2016',\n",
    "    \n",
    "    # Election data 2020\n",
    "    'votes_gop_2020', 'votes_dem_2020', 'total_votes_2020',\n",
    "    'diff_2020', 'per_gop_2020', 'per_dem_2020', 'per_point_diff_2020',\n",
    "    \n",
    "    # Winners\n",
    "    'winner_2012', 'winner_2016', 'winner_2020',\n",
    "    \n",
    "    # Race data 2012\n",
    "    'Total:_2012',\n",
    "    'pct_American Indian and Alaska Native alone_2012',\n",
    "    'pct_Asian alone_2012',\n",
    "    'pct_Black or African American alone_2012',\n",
    "    'pct_Native Hawaiian and Other Pacific Islander alone_2012',\n",
    "    'pct_Some other race alone_2012',\n",
    "    'pct_Two or more races:_2012',\n",
    "    'pct_White alone_2012',\n",
    "    \n",
    "    # Race data 2016\n",
    "    'Total:_2016',\n",
    "    'pct_American Indian and Alaska Native alone_2016',\n",
    "    'pct_Asian alone_2016',\n",
    "    'pct_Black or African American alone_2016',\n",
    "    'pct_Native Hawaiian and Other Pacific Islander alone_2016',\n",
    "    'pct_Some other race alone_2016',\n",
    "    'pct_Two or more races:_2016',\n",
    "    'pct_White alone_2016',\n",
    "    \n",
    "    # Race data 2020\n",
    "    'Total:_2020',\n",
    "    'pct_American Indian and Alaska Native alone_2020',\n",
    "    'pct_Asian alone_2020',\n",
    "    'pct_Black or African American alone_2020',\n",
    "    'pct_Native Hawaiian and Other Pacific Islander alone_2020',\n",
    "    'pct_Some other race alone_2020',\n",
    "    'pct_Two or more races:_2020',\n",
    "    'pct_White alone_2020',\n",
    "]\n",
    "\n",
    "subset_df = df2[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the per_point_dff_2016 column values from strings with percent signs to floats\n",
    "subset_df['per_point_diff_2016'] = subset_df['per_point_diff_2016'].str.replace('%', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the subsset_df , color by a value\n",
    "subset_df.plot(column='pct_White alone_2012', legend=True, figsize=(15, 15), cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all col names to a file\n",
    "with open('data/other/all_col_names.txt', 'w') as f:\n",
    "    for col in census_election_data.columns:\n",
    "        f.write(f'{col}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns that have VALUE in the name\n",
    "value = 'Metro'\n",
    "total_columns = [col for col in census_election_data.columns if value in col]\n",
    "print(total_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show count of metro 2013 vals\n",
    "print(census_election_data['Metro_2013'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the counties with the largest changes in per_gop_2012 to per_gop_2016 (already columns)\n",
    "census_election_data['gop_change'] = census_election_data['per_gop_2016'] - census_election_data['per_gop_2012']\n",
    "census_election_data['gop_change_abs'] = abs(census_election_data['gop_change'])\n",
    "census_election_data = census_election_data.sort_values('gop_change_abs', ascending=False)\n",
    "print(census_election_data[['county_name', 'gop_change', 'gop_change_abs']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot counties by Metro_2013 (0=non-metro, 1=metro)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "census_election_data.plot(column='Metro_2013', ax=ax, legend=True)\n",
    "plt.title('Metro_2013')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML / Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_election_data.dtypes.value_counts()\n",
    "\n",
    "# show all column names that are object type\n",
    "object_columns = census_election_data.select_dtypes(include='object').columns\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate correlation between all columns.  this should be returned in the form of a list with the top 100 highest ocrrelating pairs\n",
    "# correlation_matrix = census_election_data.corr().abs()\n",
    "# correlation_values = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "# correlation_values = correlation_values[correlation_values < 1]\n",
    "# correlation_values = correlation_values[0:100]\n",
    "# print(correlation_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def analyze_correlations(df, top_n=20, min_correlation=0.0):\n",
    "    \"\"\"\n",
    "    Analyze correlations between all numeric columns in a dataframe with formatted output.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe\n",
    "    top_n (int): Number of top correlations to return\n",
    "    min_correlation (float): Minimum absolute correlation value to consider\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Top correlations sorted by absolute value\n",
    "    \"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    \n",
    "    # Convert correlation matrix to long format\n",
    "    correlations = []\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            if col1 < col2:  # Only take upper triangle to avoid duplicates\n",
    "                correlation = corr_matrix.loc[col1, col2]\n",
    "                if abs(correlation) >= min_correlation:\n",
    "                    correlations.append({\n",
    "                        'Variable 1': col1,\n",
    "                        'Variable 2': col2,\n",
    "                        'Correlation': correlation,\n",
    "                        'Absolute Correlation': abs(correlation)\n",
    "                    })\n",
    "    \n",
    "    # Convert to dataframe and sort\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    corr_df = corr_df.sort_values('Absolute Correlation', ascending=False)\n",
    "    \n",
    "    # Format correlation values\n",
    "    corr_df['Correlation'] = corr_df['Correlation'].round(3)\n",
    "    \n",
    "    # Create formatted output\n",
    "    # result = corr_df.head(top_n)[['Variable 1', 'Variable 2', 'Correlation']]\n",
    "    result = corr_df[['Variable 1', 'Variable 2', 'Correlation']]\n",
    "    \n",
    "    # # Print formatted results\n",
    "    # print(\"\\nTop\", top_n, \"Correlations:\")\n",
    "    # print(\"=\" * 100)\n",
    "    # for idx, row in result.iterrows(): \n",
    "    #     print(f\"\\nCorrelation: {row['Correlation']:.3f}\")\n",
    "    #     print(f\"Variable 1: {row['Variable 1']}\")\n",
    "    #     print(f\"Variable 2: {row['Variable 2']}\")\n",
    "    #     print(\"-\" * 100)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# top_correlations = analyze_correlations(df, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_correlations = analyze_correlations(census_election_data, top_n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all perfect correlations (1 or -1)\n",
    "# top_correlations = top_correlations[top_correlations['Correlation'] < 1]\n",
    "# top_correlations = top_correlations[top_correlations['Correlation'] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the top_correlations dataframe by a string in either Variable 1 or Variable 2 (separately), as well as a correlation value range\n",
    "def filter_correlations(correlations, filter_str=None, min_correlation=None, max_correlation=None):\n",
    "    \"\"\"\n",
    "    Filter correlation dataframe by variable names and correlation values.\n",
    "    \n",
    "    Parameters:\n",
    "    correlations (pandas.DataFrame): DataFrame with correlation values\n",
    "    filter_str (str): String to filter variable names\n",
    "    min_correlation (float): Minimum correlation value\n",
    "    max_correlation (float): Maximum correlation value\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Filtered correlation dataframe\n",
    "    \"\"\"\n",
    "    # Filter by variable names\n",
    "    if filter_str:\n",
    "        correlations = correlations[\n",
    "            correlations['Variable 1'].str.contains(filter_str) | \n",
    "            correlations['Variable 2'].str.contains(filter_str)\n",
    "        ]\n",
    "    \n",
    "    # Filter by correlation values\n",
    "    if min_correlation:\n",
    "        correlations = correlations[correlations['Correlation'] >= min_correlation]\n",
    "    if max_correlation:\n",
    "        correlations = correlations[correlations['Correlation'] <= max_correlation]\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_filter = 'religion'\n",
    "min_corr = 0.7\n",
    "max_corr = 1\n",
    "filtered_correlations = filter_correlations(top_correlations, filter_str=var_filter, min_correlation=min_corr, max_correlation=max_corr)\n",
    "# filtered_correlations = filter_correlations(top_correlations, min_correlation=min_corr, max_correlation=max_corr)\n",
    "filtered_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_dataframe = pd.DataFrame(census_election_data.drop(columns=['geometry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_importances = None\n",
    "        self.selected_features = None\n",
    "        self.target = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _preprocess_data(self, df, columns_to_drop=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data by handling missing values and non-numeric columns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input dataframe\n",
    "        columns_to_drop : list\n",
    "            List of strings - columns containing any of these strings will be dropped\n",
    "        \"\"\"\n",
    "        # Create a copy of the dataframe\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Drop columns containing any of the strings in columns_to_drop\n",
    "        if columns_to_drop:\n",
    "            # Get all columns that contain any of the strings to drop\n",
    "            cols_to_drop = []\n",
    "            for pattern in columns_to_drop:\n",
    "                matching_cols = df_processed.columns[df_processed.columns.str.contains(pattern, case=False)]\n",
    "                cols_to_drop.extend(matching_cols)\n",
    "            \n",
    "            # Remove duplicates and ensure we don't drop the target\n",
    "            cols_to_drop = list(set([col for col in cols_to_drop if col != self.target]))\n",
    "            \n",
    "            # Drop the columns\n",
    "            df_processed = df_processed.drop(columns=cols_to_drop, errors='ignore')\n",
    "            \n",
    "            print(f\"Dropped {len(cols_to_drop)} columns containing the patterns: {columns_to_drop}\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        for column in df_processed.columns:\n",
    "            if df_processed[column].dtype in ['int64', 'float64']:\n",
    "                df_processed[column].fillna(df_processed[column].mean(), inplace=True)\n",
    "            else:\n",
    "                df_processed[column].fillna(df_processed[column].mode()[0], inplace=True)\n",
    "        \n",
    "        # Get numeric columns only, making sure to include the target column\n",
    "        numeric_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if self.target not in numeric_cols:\n",
    "            numeric_cols = numeric_cols.append(pd.Index([self.target]))\n",
    "        \n",
    "        return df_processed[numeric_cols]\n",
    "\n",
    "    def analyze_importance(self, df, target_column, columns_to_drop=None, n_features=20):\n",
    "        \"\"\"\n",
    "        Analyze feature importance for a given target column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input dataframe\n",
    "        target_column : str\n",
    "            Name of the target column\n",
    "        columns_to_drop : list\n",
    "            List of strings - columns containing any of these strings will be dropped\n",
    "        n_features : int, optional\n",
    "            Number of top features to return (default=20)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing:\n",
    "            - feature_importances: DataFrame with feature importances\n",
    "            - model_performance: Dict with R² and RMSE scores\n",
    "            - model: Trained RandomForestRegressor\n",
    "        \"\"\"\n",
    "        # Store target\n",
    "        self.target = target_column\n",
    "        \n",
    "        # Preprocess data\n",
    "        df_processed = self._preprocess_data(df, columns_to_drop)\n",
    "        \n",
    "        # Verify target column is present\n",
    "        if target_column not in df_processed.columns:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found in processed data\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df_processed.drop(columns=[target_column])\n",
    "        y = df_processed[target_column]\n",
    "        \n",
    "        # Store selected features\n",
    "        self.selected_features = X.columns.tolist()\n",
    "        \n",
    "        # Print number of features being used\n",
    "        print(f\"Using {len(self.selected_features)} features for analysis\")\n",
    "        \n",
    "        # Split the data\n",
    "        print(\"Splitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale the features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        self.model = RandomForestRegressor(n_estimators=100, random_state=42, verbose=1, n_jobs=-1)\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = self.model.feature_importances_\n",
    "        self.feature_importances = pd.DataFrame({\n",
    "            'feature': self.selected_features,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False).head(n_features)\n",
    "        \n",
    "        return {\n",
    "            'feature_importances': self.feature_importances,\n",
    "            'model_performance': {\n",
    "                'r2_score': r2,\n",
    "                'rmse': rmse\n",
    "            },\n",
    "            'model': self.model\n",
    "        }\n",
    "    \n",
    "    def plot_importance(self, n_features=20):\n",
    "        \"\"\"Plot feature importance using matplotlib.\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(self.feature_importances['feature'][:n_features],\n",
    "                    self.feature_importances['importance'][:n_features])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top {n_features} Most Important Features for {self.target}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"matplotlib is required for plotting. Please install it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analyzer instance\n",
    "analyzer = FeatureImportanceAnalyzer()\n",
    "\n",
    "# Define patterns to drop - any column containing these strings will be dropped\n",
    "columns_to_drop = ['vote', 'winner', 'per']\n",
    "\n",
    "# Analyze importance for a target variable\n",
    "results = analyzer.analyze_importance(\n",
    "    df=county_dataframe,\n",
    "    target_column='winner_2016_numeric',\n",
    "    columns_to_drop=columns_to_drop\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(results['feature_importances'].head(10))\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"R² Score: {results['model_performance']['r2_score']:.4f}\")\n",
    "print(f\"RMSE: {results['model_performance']['rmse']:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "analyzer.plot_importance(n_features=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns with religion in the name\n",
    "religion_columns = [col for col in county_dataframe.columns if 'muslim' in col]\n",
    "print(religion_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show row where religion_jehovahs_witnesses_percent_adherents_of_total_population is max\n",
    "max_row = county_dataframe[county_dataframe['religion_jehovahs_witnesses_percent_adherents_of_total_population'] == county_dataframe['religion_jehovahs_witnesses_percent_adherents_of_total_population'].max()]\n",
    "max_row.county_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_row['Total:_2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "\n",
    "def plot_choropleth(\n",
    "    gdf,\n",
    "    feature_name,\n",
    "    scale='linear',\n",
    "    figsize=(15, 10),\n",
    "    cmap='YlOrRd',\n",
    "    title=None,\n",
    "    legend_label=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a choropleth map from a GeoDataFrame with flexible scaling options.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The geodataframe containing geographic and feature data\n",
    "    feature_name : str\n",
    "        The column name of the feature to plot\n",
    "    scale : str, default 'linear'\n",
    "        The scale type ('linear' or 'log')\n",
    "    figsize : tuple, default (15, 10)\n",
    "        Figure size as (width, height)\n",
    "    cmap : str, default 'YlOrRd'\n",
    "        Matplotlib colormap name\n",
    "    title : str, optional\n",
    "        Custom title for the plot. If None, will generate based on feature name\n",
    "    legend_label : str, optional\n",
    "        Custom label for the legend. If None, will use feature name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax : tuple\n",
    "        The figure and axis objects for further customization if needed\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if feature_name not in gdf.columns:\n",
    "        raise ValueError(f\"Feature '{feature_name}' not found in the GeoDataFrame\")\n",
    "    \n",
    "    if scale not in ['linear', 'log']:\n",
    "        raise ValueError(\"Scale must be either 'linear' or 'log'\")\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    # Prepare the normalization\n",
    "    if scale == 'log':\n",
    "        # Handle zero or negative values for log scale\n",
    "        min_val = gdf[feature_name].min()\n",
    "        if min_val <= 0:\n",
    "            print(f\"Warning: Data contains values ≤ 0. Adding offset for log scale.\")\n",
    "            gdf = gdf.copy()\n",
    "            gdf[feature_name] = gdf[feature_name] - min_val + 1\n",
    "        \n",
    "        norm = LogNorm(\n",
    "            vmin=gdf[feature_name].min(),\n",
    "            vmax=gdf[feature_name].max()\n",
    "        )\n",
    "    else:\n",
    "        norm = None\n",
    "    \n",
    "    # Generate default title and legend label if not provided\n",
    "    if title is None:\n",
    "        title = f\"{feature_name} by County\"\n",
    "        if scale == 'log':\n",
    "            title += \" (Log Scale)\"\n",
    "            \n",
    "    if legend_label is None:\n",
    "        legend_label = feature_name\n",
    "        if scale == 'log':\n",
    "            legend_label += \" (log scale)\"\n",
    "    \n",
    "    # Create the plot\n",
    "    gdf.plot(\n",
    "        column=feature_name,\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            'label': legend_label,\n",
    "            'orientation': 'vertical'\n",
    "        },\n",
    "        cmap=cmap,\n",
    "        norm=norm\n",
    "    )\n",
    "    \n",
    "    # Customize the appearance\n",
    "    plt.title(title, pad=20)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choropleth(census_election_data, 'winner_2024_numeric', scale='linear', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi feature Choropleth (folium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "from branca.colormap import LinearColormap\n",
    "import pandas as pd\n",
    "\n",
    "# Define color schemes\n",
    "COLOR_SCHEMES = {\n",
    "    'population': ['#fee5d9', '#fcae91', '#fb6a4a', '#de2d26', '#a50f15'],  # Reds\n",
    "    'political': ['#0571b0', '#92c5de', '#f7f7f7', '#f4a582', '#ca0020'],   # Blue-Red diverging\n",
    "    'sequential': ['#ffffcc', '#a1dab4', '#41b6c4', '#2c7fb8', '#253494'],  # Blue-Green\n",
    "    'viridis': ['#440154', '#414487', '#2a788e', '#22a884', '#7ad151', '#fde725']  # Viridis\n",
    "}\n",
    "\n",
    "def create_multi_feature_choropleth(\n",
    "    gdf,\n",
    "    feature_configs,\n",
    "    center=None,\n",
    "    zoom_start=6,\n",
    "    width='100%',\n",
    "    height='100%'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map with multiple togglable feature layers using Folium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The geodataframe containing geographic and feature data\n",
    "    feature_configs : list of dict\n",
    "        List of feature configurations, each containing:\n",
    "        {\n",
    "            'name': str (column name in gdf),\n",
    "            'scale': str ('linear' or 'log'),\n",
    "            'color_scheme': str (optional, one of 'population', 'political', 'sequential'),\n",
    "            'legend_name': str (optional, display name for the legend)\n",
    "        }\n",
    "    center : tuple, optional\n",
    "        (lat, lon) center coordinates. If None, will use centroid of the data\n",
    "    zoom_start : int, default 6\n",
    "        Initial zoom level\n",
    "    width : str, default '100%'\n",
    "        Width of the map in percentage\n",
    "    height : str, default '100%'\n",
    "        Height of the map in percentage\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map\n",
    "        The interactive map object\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    for config in feature_configs:\n",
    "        if config['name'] not in gdf.columns:\n",
    "            raise ValueError(f\"Feature '{config['name']}' not found in the GeoDataFrame\")\n",
    "        if config.get('scale') not in ['linear', 'log']:\n",
    "            raise ValueError(f\"Scale for feature '{config['name']}' must be 'linear' or 'log'\")\n",
    "    \n",
    "    # Calculate center if not provided\n",
    "    if center is None:\n",
    "        center = [\n",
    "            gdf.geometry.centroid.y.mean(),\n",
    "            gdf.geometry.centroid.x.mean()\n",
    "        ]\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(\n",
    "        location=center,\n",
    "        zoom_start=zoom_start,\n",
    "        width=width,\n",
    "        height=height\n",
    "    )\n",
    "    \n",
    "    # Create feature groups dictionary\n",
    "    feature_groups = {}\n",
    "    \n",
    "    for config in feature_configs:\n",
    "        feature_name = config['name']\n",
    "        scale_type = config['scale']\n",
    "        # Use viridis for log scale, otherwise use specified color scheme\n",
    "        color_scheme = 'viridis' if scale_type == 'log' else config.get('color_scheme', 'sequential')\n",
    "        legend_name = config.get('legend_name', feature_name)\n",
    "        \n",
    "        # Get appropriate color scheme\n",
    "        colors = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES['sequential'])\n",
    "        \n",
    "        # Create feature group\n",
    "        fg = folium.FeatureGroup(name=legend_name, show=False)\n",
    "        \n",
    "        # Handle data scaling and preparation\n",
    "        data = gdf[feature_name].copy()\n",
    "        if scale_type == 'log':\n",
    "            min_val = data.min()\n",
    "            if min_val <= 0:\n",
    "                print(f\"Warning: {feature_name} contains values ≤ 0. Adding offset for log scale.\")\n",
    "                data = data - min_val + 1\n",
    "            # Apply log transformation\n",
    "            data = np.log(data)\n",
    "        \n",
    "        # Create colormap\n",
    "        colormap = LinearColormap(\n",
    "            colors=colors,\n",
    "            vmin=data.min(),\n",
    "            vmax=data.max(),\n",
    "            caption=legend_name + (' (log scale)' if scale_type == 'log' else '')\n",
    "        )\n",
    "        \n",
    "        # Add choropleth layer\n",
    "        for idx, row in gdf.iterrows():\n",
    "            value = row[feature_name]\n",
    "            if scale_type == 'log':\n",
    "                if value <= 0:\n",
    "                    value = value - min_val + 1\n",
    "                value = np.log(value)\n",
    "                \n",
    "            color = colormap(value)\n",
    "            \n",
    "            # Format display value\n",
    "            display_value = row[feature_name]\n",
    "            if isinstance(display_value, (int, float)):\n",
    "                if abs(display_value) >= 1000000:\n",
    "                    display_value = f\"{display_value/1000000:.2f}M\"\n",
    "                elif abs(display_value) >= 1000:\n",
    "                    display_value = f\"{display_value/1000:.2f}K\"\n",
    "                elif feature_name.startswith('per_'):  # Percentage values\n",
    "                    display_value = f\"{display_value:.2f}%\"\n",
    "                else:\n",
    "                    display_value = f\"{display_value:.2f}\"\n",
    "            \n",
    "            # Get state and county names\n",
    "            state_name = row.get('state_name', row.get('STATE_NAME', ''))\n",
    "            county_name = row.get('county_name', row.get('COUNTY_NAME', ''))\n",
    "            \n",
    "            # Create GeoJSON-style feature\n",
    "            feature = {\n",
    "                'type': 'Feature',\n",
    "                'geometry': row.geometry.__geo_interface__,\n",
    "                'properties': {\n",
    "                    'value': display_value,\n",
    "                    'state': state_name,\n",
    "                    'county': county_name\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add polygon to feature group\n",
    "            folium.GeoJson(\n",
    "                feature,\n",
    "                style_function=lambda x, color=color: {\n",
    "                    'fillColor': color,\n",
    "                    'color': 'black',\n",
    "                    'weight': 1,\n",
    "                    'fillOpacity': 0.7\n",
    "                },\n",
    "                tooltip=folium.GeoJsonTooltip(\n",
    "                    fields=['county', 'state', 'value'],\n",
    "                    aliases=['County', 'State', legend_name],\n",
    "                    localize=True,\n",
    "                    sticky=False,\n",
    "                    labels=True,\n",
    "                    style=\"\"\"\n",
    "                        background-color: white;\n",
    "                        border: 2px solid black;\n",
    "                        border-radius: 3px;\n",
    "                        box-shadow: 3px 3px 3px rgba(0,0,0,0.2);\n",
    "                        padding: 5px;\n",
    "                        font-size: 12px;\n",
    "                    \"\"\"\n",
    "                )\n",
    "            ).add_to(fg)\n",
    "        \n",
    "        # Add colormap to map\n",
    "        colormap.add_to(m)\n",
    "        \n",
    "        # Add feature group to map\n",
    "        fg.add_to(m)\n",
    "        \n",
    "        feature_groups[legend_name] = fg\n",
    "    \n",
    "    # Add custom JavaScript for radio button behavior\n",
    "    layer_control_html = \"\"\"\n",
    "    <script type=\"text/javascript\">\n",
    "    function setupLayerControl() {\n",
    "        // Get layer control container\n",
    "        var layerControlContainer = document.querySelector('.leaflet-control-layers-overlays');\n",
    "        if (!layerControlContainer) {\n",
    "            setTimeout(setupLayerControl, 100);\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        // Replace checkboxes with radio buttons\n",
    "        var inputs = layerControlContainer.querySelectorAll('input[type=\"checkbox\"]');\n",
    "        inputs.forEach(function(input) {\n",
    "            input.type = 'radio';\n",
    "            input.name = 'layer-control';\n",
    "            \n",
    "            // Update the input's checked state based on layer visibility\n",
    "            var layer = input._layer;\n",
    "            if (layer && layer._map) {\n",
    "                input.checked = layer._map.hasLayer(layer);\n",
    "            }\n",
    "            \n",
    "            // Add change listener to handle layer toggling\n",
    "            input.addEventListener('change', function(e) {\n",
    "                if (this.checked) {\n",
    "                    // Uncheck and hide all other layers\n",
    "                    inputs.forEach(function(otherInput) {\n",
    "                        if (otherInput !== input) {\n",
    "                            otherInput.checked = false;\n",
    "                            if (otherInput._layer) {\n",
    "                                otherInput._layer.remove();\n",
    "                            }\n",
    "                        }\n",
    "                    });\n",
    "                    \n",
    "                    // Show the selected layer\n",
    "                    if (this._layer) {\n",
    "                        this._layer.addTo(this._layer._map);\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        });\n",
    "\n",
    "        // Monitor layer visibility changes\n",
    "        var map = document.querySelector('#map');  // Assuming map has id=\"map\"\n",
    "        if (map && map._leaflet_map) {\n",
    "            map._leaflet_map.on('layeradd layerremove', function(e) {\n",
    "                inputs.forEach(function(input) {\n",
    "                    if (input._layer === e.layer) {\n",
    "                        input.checked = e.type === 'layeradd';\n",
    "                    }\n",
    "                });\n",
    "            });\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Initialize when DOM is ready\n",
    "    if (document.readyState === 'loading') {\n",
    "        document.addEventListener('DOMContentLoaded', setupLayerControl);\n",
    "    } else {\n",
    "        setupLayerControl();\n",
    "    }\n",
    "\n",
    "    // Also set up observer to handle dynamic updates\n",
    "    var observer = new MutationObserver(function(mutations) {\n",
    "        mutations.forEach(function(mutation) {\n",
    "            if (mutation.addedNodes.length) {\n",
    "                setupLayerControl();\n",
    "            }\n",
    "        });\n",
    "    });\n",
    "\n",
    "    observer.observe(document.body, {\n",
    "        childList: true,\n",
    "        subtree: true\n",
    "    });\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add layer control to map\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Add custom JavaScript to handle radio button behavior\n",
    "    m.get_root().html.add_child(folium.Element(layer_control_html))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_configs = [\n",
    "    {\n",
    "        'name': 'INDUSTRY_agriculture_forestry_fishing_and_hunting_and_mining_percent_2020',\n",
    "        'scale': 'linear',\n",
    "        'color_scheme': 'political',\n",
    "        'legend_name': 'Agriculture, Forestry, Fishing, Mining Percent 2020'\n",
    "    },\n",
    "    {\n",
    "        'name': 'per_gop_2020',\n",
    "        'scale': 'linear',\n",
    "        'color_scheme': 'political',\n",
    "        'legend_name': 'GOP Vote Share 2020'\n",
    "    }\n",
    "]\n",
    "\n",
    "map_obj = create_multi_feature_choropleth(\n",
    "    gdf=census_election_data,\n",
    "    feature_configs=feature_configs,\n",
    "    zoom_start=6\n",
    ")\n",
    "map_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Election results all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from branca.colormap import LinearColormap\n",
    "import numpy as np\n",
    "\n",
    "def create_multiyear_election_map_folium(\n",
    "    gdf,\n",
    "    years=[2012, 2016, 2020, 2024],\n",
    "    center_lat=39.8283,\n",
    "    center_lon=-98.5795,\n",
    "    zoom_start=4,\n",
    "    fill_opacity=0.7,\n",
    "    default_year=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive election map with mutually exclusive year layers\n",
    "    controlled by radio buttons.\n",
    "    \"\"\"\n",
    "    if default_year is None:\n",
    "        default_year = max(years)\n",
    "    \n",
    "    required_columns = ['geometry', 'fips', 'county_name', 'state_name']\n",
    "    for year in years:\n",
    "        required_columns.extend([f'per_dem_{year}', f'per_gop_{year}'])\n",
    "    \n",
    "    gdf = gdf[required_columns].copy()\n",
    "    \n",
    "    # Calculate margins and find global scale\n",
    "    max_diff = float('-inf')\n",
    "    for year in years:\n",
    "        dem_col = f'per_dem_{year}'\n",
    "        gop_col = f'per_gop_{year}'\n",
    "        margin_col = f'margin_{year}'\n",
    "        gdf[margin_col] = gdf[gop_col] - gdf[dem_col]\n",
    "        year_max_diff = max(abs(gdf[margin_col].min()), abs(gdf[margin_col].max()))\n",
    "        max_diff = max(max_diff, year_max_diff)\n",
    "    \n",
    "    max_diff = np.ceil(max_diff * 10) / 10\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=zoom_start,\n",
    "        tiles='cartodbpositron'\n",
    "    )\n",
    "    \n",
    "    # Create common colormap\n",
    "    colormap = LinearColormap(\n",
    "        colors=[\n",
    "            '#0000CC',  # Strong Democratic (dark blue)\n",
    "            '#2A2AFF',  # Democratic\n",
    "            '#5555FF',  # Lean Democratic\n",
    "            '#FFFFFF',  # Neutral\n",
    "            '#FF5555',  # Lean Republican\n",
    "            '#FF2A2A',  # Republican\n",
    "            '#CC0000'   # Strong Republican (dark red)\n",
    "        ],\n",
    "        vmin=-max_diff,\n",
    "        vmax=max_diff,\n",
    "        caption='Party Vote Share Margin (Rep - Dem)'\n",
    "    )\n",
    "    \n",
    "    # Create layers\n",
    "    for year in years:\n",
    "        dem_col = f'per_dem_{year}'\n",
    "        gop_col = f'per_gop_{year}'\n",
    "        margin_col = f'margin_{year}'\n",
    "        \n",
    "        def make_style_function(margin_column):\n",
    "            def style_function(feature):\n",
    "                value = feature['properties'][margin_column]\n",
    "                if value is None:\n",
    "                    return {'fillColor': '#black', 'fillOpacity': 0, 'weight': 0}\n",
    "                return {\n",
    "                    'fillColor': colormap(value),\n",
    "                    'fillOpacity': fill_opacity,\n",
    "                    'color': 'none',\n",
    "                    'weight': 0\n",
    "                }\n",
    "            return style_function\n",
    "        \n",
    "        def highlight_function(feature):\n",
    "            return {\n",
    "                'fillOpacity': fill_opacity,\n",
    "                'color': 'none',\n",
    "                'weight': 0\n",
    "            }\n",
    "        \n",
    "        # Create GeoJson layer for this year\n",
    "        geojson = folium.GeoJson(\n",
    "            data=gdf.to_crs(epsg='4326').__geo_interface__,\n",
    "            name=str(year),\n",
    "            style_function=make_style_function(margin_col),\n",
    "            highlight_function=highlight_function,\n",
    "            show=(year == default_year),  # Show only default year\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['county_name', 'state_name', gop_col, dem_col, margin_col],\n",
    "                aliases=['County', 'State', \n",
    "                        f'{year} Republican',\n",
    "                        f'{year} Democratic',\n",
    "                        f'{year} Rep-Dem Margin'],\n",
    "                localize=True,\n",
    "                sticky=False,\n",
    "                labels=True,\n",
    "                style=\"\"\"\n",
    "                    background-color: white;\n",
    "                    border: none;\n",
    "                    border-radius: 3px;\n",
    "                    box-shadow: 3px 3px 10px rgba(0,0,0,0.2);\n",
    "                    font-size: 12px;\n",
    "                    padding: 8px;\n",
    "                \"\"\"\n",
    "            )\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Add colormap\n",
    "    colormap.add_to(m)\n",
    "    \n",
    "    # Add layer control with radio buttons\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "    \n",
    "    # Add JavaScript to ensure mutual exclusivity and visibility\n",
    "    m.get_root().html.add_child(folium.Element(\"\"\"\n",
    "        <script>\n",
    "        document.addEventListener('DOMContentLoaded', function() {\n",
    "            // Get all layer control inputs\n",
    "            var inputs = document.querySelectorAll('.leaflet-control-layers-selector');\n",
    "            \n",
    "            // Initially check the default year\n",
    "            var defaultChecked = false;\n",
    "            inputs.forEach(function(input) {\n",
    "                if (!defaultChecked) {\n",
    "                    input.checked = true;\n",
    "                    defaultChecked = true;\n",
    "                }\n",
    "            });\n",
    "            \n",
    "            // Add click handler to each input\n",
    "            inputs.forEach(function(input) {\n",
    "                input.addEventListener('change', function() {\n",
    "                    if (this.checked) {\n",
    "                        // Uncheck all other inputs\n",
    "                        inputs.forEach(function(otherInput) {\n",
    "                            if (otherInput !== input) {\n",
    "                                otherInput.checked = false;\n",
    "                            }\n",
    "                        });\n",
    "                    } else {\n",
    "                        // Ensure at least one layer is always checked\n",
    "                        var anyChecked = false;\n",
    "                        inputs.forEach(function(otherInput) {\n",
    "                            if (otherInput.checked) anyChecked = true;\n",
    "                        });\n",
    "                        if (!anyChecked) this.checked = true;\n",
    "                    }\n",
    "                });\n",
    "            });\n",
    "        });\n",
    "        </script>\n",
    "    \"\"\"))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = create_multiyear_election_map_folium(\n",
    "    census_election_data,\n",
    "    years=[2012, 2016, 2020, 2024],\n",
    "    default_year=2024  # Optional, defaults to most recent year\n",
    ")\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from libpysal.weights import Queen, KNN\n",
    "from esda.moran import Moran, Moran_Local\n",
    "import warnings\n",
    "from typing import Union, Tuple, Dict\n",
    "import mapclassify\n",
    "\n",
    "def analyze_spatial_correlation(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    column: str,\n",
    "    weight_type: str = 'queen',\n",
    "    k_neighbors: int = 5,\n",
    "    title: str = None,\n",
    "    figsize: Tuple[int, int] = (20, 10)\n",
    ") -> Tuple[Dict, plt.Figure, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes spatial correlation for any column in a GeoDataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing geometry and data\n",
    "    column : str\n",
    "        Name of column to analyze\n",
    "    weight_type : str, optional (default='queen')\n",
    "        Type of spatial weights to use ('queen' or 'knn')\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of neighbors for KNN weights\n",
    "    title : str, optional\n",
    "        Custom title for plots\n",
    "    figsize : tuple, optional (default=(20, 10))\n",
    "        Figure size for plots\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "    if column not in gdf.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in GeoDataFrame\")\n",
    "    \n",
    "    # Create copy to avoid modifying original\n",
    "    gdf_analysis = gdf.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if gdf_analysis[column].isnull().any():\n",
    "        print(f\"Warning: {gdf_analysis[column].isnull().sum()} missing values found and will be removed\")\n",
    "        gdf_analysis = gdf_analysis.dropna(subset=[column])\n",
    "    \n",
    "    # Create spatial weights matrix\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if weight_type.lower() == 'queen':\n",
    "            weights = Queen.from_dataframe(gdf_analysis, use_index=True)\n",
    "        elif weight_type.lower() == 'knn':\n",
    "            weights = KNN.from_dataframe(gdf_analysis, k=k_neighbors)\n",
    "        else:\n",
    "            raise ValueError(\"weight_type must be either 'queen' or 'knn'\")\n",
    "    \n",
    "    # Handle islands if using queen weights\n",
    "    if weight_type.lower() == 'queen':\n",
    "        islands = weights.islands\n",
    "        if len(islands) > 0:\n",
    "            print(f\"Removing {len(islands)} isolated areas from analysis\")\n",
    "            gdf_analysis = gdf_analysis[~gdf_analysis.index.isin(islands)].copy()\n",
    "            weights = Queen.from_dataframe(gdf_analysis, use_index=True)\n",
    "    \n",
    "    weights.transform = 'r'  # Row-standardize weights\n",
    "    \n",
    "    # Calculate Global Moran's I\n",
    "    moran = Moran(gdf_analysis[column], weights)\n",
    "    \n",
    "    # Calculate Local Moran's I\n",
    "    local_moran = Moran_Local(gdf_analysis[column], weights)\n",
    "    \n",
    "    # Add local indicators to dataframe\n",
    "    gdf_analysis['local_moran_i'] = local_moran.Is\n",
    "    gdf_analysis['local_moran_p'] = local_moran.p_sim\n",
    "    \n",
    "    # Classify clusters\n",
    "    gdf_analysis['cluster_type'] = 'Not Significant'\n",
    "    sig_mask = local_moran.p_sim < 0.05\n",
    "    \n",
    "    # Standardize values for clustering\n",
    "    std_val = (gdf_analysis[column] - gdf_analysis[column].mean()) / gdf_analysis[column].std()\n",
    "    lag_val = weights.sparse.dot(std_val)\n",
    "    \n",
    "    # Assign cluster types\n",
    "    gdf_analysis.loc[sig_mask & (std_val > 0) & (lag_val > 0), 'cluster_type'] = 'High-High'\n",
    "    gdf_analysis.loc[sig_mask & (std_val < 0) & (lag_val < 0), 'cluster_type'] = 'Low-Low'\n",
    "    gdf_analysis.loc[sig_mask & (std_val > 0) & (lag_val < 0), 'cluster_type'] = 'High-Low'\n",
    "    gdf_analysis.loc[sig_mask & (std_val < 0) & (lag_val > 0), 'cluster_type'] = 'Low-High'\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Original Values\n",
    "    plot1 = gdf_analysis.plot(\n",
    "        column=column,\n",
    "        ax=axes[0],\n",
    "        legend=True,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[0].set_title(f\"Distribution of {column}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Add colorbar with label\n",
    "    cbar1 = plt.colorbar(plot1.get_children()[0], ax=axes[0])\n",
    "    cbar1.set_label(column, rotation=270, labelpad=15)\n",
    "    \n",
    "    # Plot 2: Local Moran's I Values\n",
    "    plot2 = gdf_analysis.plot(\n",
    "        column='local_moran_i',\n",
    "        ax=axes[1],\n",
    "        legend=True,\n",
    "        cmap='RdBu'\n",
    "    )\n",
    "    axes[1].set_title(\"Local Moran's I Values\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Add colorbar with label\n",
    "    cbar2 = plt.colorbar(plot2.get_children()[0], ax=axes[1])\n",
    "    cbar2.set_label(\"Local Moran's I\", rotation=270, labelpad=15)\n",
    "    \n",
    "    # Plot 3: Cluster Types with categorical legend\n",
    "    cluster_colors = {'High-High': '#d7191c', \n",
    "                     'Low-Low': '#2c7bb6', \n",
    "                     'High-Low': '#fdae61', \n",
    "                     'Low-High': '#abd9e9', \n",
    "                     'Not Significant': '#ffffbf'}\n",
    "    \n",
    "    for cluster_type, color in cluster_colors.items():\n",
    "        mask = gdf_analysis['cluster_type'] == cluster_type\n",
    "        if mask.any():\n",
    "            gdf_analysis[mask].plot(\n",
    "                color=color,\n",
    "                ax=axes[2],\n",
    "                label=cluster_type\n",
    "            )\n",
    "    \n",
    "    axes[2].set_title('Spatial Clusters')\n",
    "    axes[2].axis('off')\n",
    "    axes[2].legend(title='Cluster Type', bbox_to_anchor=(1.3, 1))\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16, y=1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results = {\n",
    "        'global_statistics': {\n",
    "            'morans_i': moran.I,\n",
    "            'p_value': moran.p_sim,\n",
    "            'z_score': moran.z_sim\n",
    "        },\n",
    "        'cluster_summary': gdf_analysis['cluster_type'].value_counts().to_dict(),\n",
    "        'local_statistics': {\n",
    "            'mean_local_i': gdf_analysis['local_moran_i'].mean(),\n",
    "            'significant_clusters': (gdf_analysis['local_moran_p'] < 0.05).sum(),\n",
    "            'percent_significant': (gdf_analysis['local_moran_p'] < 0.05).mean() * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, fig, gdf_analysis\n",
    "\n",
    "def print_spatial_correlation_results(results: Dict, column_name: str = \"selected variable\"):\n",
    "    \"\"\"\n",
    "    Prints formatted spatial correlation analysis results.\n",
    "    \"\"\"\n",
    "    print(f\"Spatial Correlation Analysis Results for {column_name}\\n\")\n",
    "    \n",
    "    print(\"Global Moran's I Statistics:\")\n",
    "    print(f\"Moran's I: {results['global_statistics']['morans_i']:.3f}\")\n",
    "    print(f\"P-value: {results['global_statistics']['p_value']:.3f}\")\n",
    "    print(f\"Z-score: {results['global_statistics']['z_score']:.3f}\")\n",
    "    \n",
    "    print(\"\\nLocal Statistics:\")\n",
    "    print(f\"Mean Local Moran's I: {results['local_statistics']['mean_local_i']:.3f}\")\n",
    "    print(f\"Number of Significant Clusters: {results['local_statistics']['significant_clusters']}\")\n",
    "    print(f\"Percent Significant: {results['local_statistics']['percent_significant']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nCluster Type Distribution:\")\n",
    "    for cluster_type, count in results['cluster_summary'].items():\n",
    "        print(f\"{cluster_type}: {count} areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, fig, gdf_analyzed = analyze_spatial_correlation(\n",
    "    gdf=census_election_data,\n",
    "    column='religion_evangelical_lutheran_church_in_america_percent_adherents_of_total_adherents',\n",
    "    title='Evangelical Lutheran Church Adherents Analysis'\n",
    ")\n",
    "\n",
    "print_spatial_correlation_results(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enhanced Spatial correlation map (folium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from libpysal.weights import Queen, KNN\n",
    "from esda.moran import Moran, Moran_Local\n",
    "import warnings\n",
    "from typing import Union, Tuple, Dict\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "def analyze_spatial_correlation_interactive(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    column: str,\n",
    "    weight_type: str = 'queen',\n",
    "    k_neighbors: int = 5,\n",
    "    center: Tuple[float, float] = None,\n",
    "    zoom_start: int = 4\n",
    ") -> Tuple[Dict, folium.Map]:\n",
    "    \"\"\"\n",
    "    Creates an interactive Folium map showing spatial correlation analysis with three layers:\n",
    "    1. Original value distribution\n",
    "    2. Local Moran's I values\n",
    "    3. Cluster types\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing geometry and data\n",
    "    column : str\n",
    "        Name of column to analyze\n",
    "    weight_type : str, optional (default='queen')\n",
    "        Type of spatial weights to use ('queen' or 'knn')\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of neighbors for KNN weights\n",
    "    center : tuple, optional\n",
    "        (lat, lon) center coordinates for the map\n",
    "    zoom_start : int, optional (default=4)\n",
    "        Initial zoom level for the map\n",
    "    \"\"\"\n",
    "    # Input validation and data preparation\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "    if column not in gdf.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in GeoDataFrame\")\n",
    "    \n",
    "    # Create copy to avoid modifying original\n",
    "    gdf_analysis = gdf.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if gdf_analysis[column].isnull().any():\n",
    "        print(f\"Warning: {gdf_analysis[column].isnull().sum()} missing values found and will be removed\")\n",
    "        gdf_analysis = gdf_analysis.dropna(subset=[column])\n",
    "    \n",
    "    # Create spatial weights matrix\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if weight_type.lower() == 'queen':\n",
    "            weights = Queen.from_dataframe(gdf_analysis, use_index=True)\n",
    "        elif weight_type.lower() == 'knn':\n",
    "            weights = KNN.from_dataframe(gdf_analysis, k=k_neighbors)\n",
    "        else:\n",
    "            raise ValueError(\"weight_type must be either 'queen' or 'knn'\")\n",
    "    \n",
    "    # Handle islands if using queen weights\n",
    "    if weight_type.lower() == 'queen':\n",
    "        islands = weights.islands\n",
    "        if len(islands) > 0:\n",
    "            print(f\"Removing {len(islands)} isolated areas from analysis\")\n",
    "            gdf_analysis = gdf_analysis[~gdf_analysis.index.isin(islands)].copy()\n",
    "            weights = Queen.from_dataframe(gdf_analysis, use_index=True)\n",
    "    \n",
    "    weights.transform = 'r'  # Row-standardize weights\n",
    "    \n",
    "    # Calculate Global Moran's I\n",
    "    moran = Moran(gdf_analysis[column], weights)\n",
    "    \n",
    "    # Calculate Local Moran's I\n",
    "    local_moran = Moran_Local(gdf_analysis[column], weights)\n",
    "    \n",
    "    # Add local indicators to dataframe\n",
    "    gdf_analysis['local_moran_i'] = local_moran.Is\n",
    "    gdf_analysis['local_moran_p'] = local_moran.p_sim\n",
    "    \n",
    "    # Classify clusters\n",
    "    gdf_analysis['cluster_type'] = 'Not Significant'\n",
    "    sig_mask = local_moran.p_sim < 0.05\n",
    "    \n",
    "    # Standardize values for clustering\n",
    "    std_val = (gdf_analysis[column] - gdf_analysis[column].mean()) / gdf_analysis[column].std()\n",
    "    lag_val = weights.sparse.dot(std_val)\n",
    "    \n",
    "    # Assign cluster types\n",
    "    gdf_analysis.loc[sig_mask & (std_val > 0) & (lag_val > 0), 'cluster_type'] = 'High-High'\n",
    "    gdf_analysis.loc[sig_mask & (std_val < 0) & (lag_val < 0), 'cluster_type'] = 'Low-Low'\n",
    "    gdf_analysis.loc[sig_mask & (std_val > 0) & (lag_val < 0), 'cluster_type'] = 'High-Low'\n",
    "    gdf_analysis.loc[sig_mask & (std_val < 0) & (lag_val > 0), 'cluster_type'] = 'Low-High'\n",
    "    \n",
    "    # Calculate map center if not provided\n",
    "    if center is None:\n",
    "        center = [\n",
    "            gdf_analysis.geometry.centroid.y.mean(),\n",
    "            gdf_analysis.geometry.centroid.x.mean()\n",
    "        ]\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=center, zoom_start=zoom_start)\n",
    "    \n",
    "    # Create feature groups for each layer\n",
    "    fg_original = folium.FeatureGroup(name=f\"Distribution of {column}\")\n",
    "    fg_moran = folium.FeatureGroup(name=\"Local Moran's I Values\")\n",
    "    fg_clusters = folium.FeatureGroup(name=\"Spatial Clusters\")\n",
    "    \n",
    "    # Create colormaps\n",
    "    colormap_original = LinearColormap(\n",
    "        colors=['#440154', '#414487', '#2a788e', '#22a884', '#7ad151', '#fde725'],\n",
    "        vmin=gdf_analysis[column].min(),\n",
    "        vmax=gdf_analysis[column].max(),\n",
    "        caption=f\"Distribution of {column}\"\n",
    "    )\n",
    "    \n",
    "    colormap_moran = LinearColormap(\n",
    "        colors=['#ca0020', '#f4a582', '#f7f7f7', '#92c5de', '#0571b0'],\n",
    "        vmin=gdf_analysis['local_moran_i'].min(),\n",
    "        vmax=gdf_analysis['local_moran_i'].max(),\n",
    "        caption=\"Local Moran's I Values\"\n",
    "    )\n",
    "    \n",
    "    cluster_colors = {\n",
    "        'High-High': '#d7191c',\n",
    "        'Low-Low': '#2c7bb6',\n",
    "        'High-Low': '#fdae61',\n",
    "        'Low-High': '#abd9e9',\n",
    "        'Not Significant': '#ffffbf'\n",
    "    }\n",
    "    \n",
    "    # Add layers\n",
    "    # 1. Original Distribution Layer\n",
    "    for idx, row in gdf_analysis.iterrows():\n",
    "        color = colormap_original(row[column])\n",
    "        folium.GeoJson(\n",
    "            row.geometry.__geo_interface__,\n",
    "            style_function=lambda x, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.7\n",
    "            },\n",
    "            tooltip=f\"{column}: {row[column]:.2f}\"\n",
    "        ).add_to(fg_original)\n",
    "    \n",
    "    # 2. Local Moran's I Layer\n",
    "    for idx, row in gdf_analysis.iterrows():\n",
    "        color = colormap_moran(row['local_moran_i'])\n",
    "        folium.GeoJson(\n",
    "            row.geometry.__geo_interface__,\n",
    "            style_function=lambda x, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.7\n",
    "            },\n",
    "            tooltip=f\"Local Moran's I: {row['local_moran_i']:.3f}<br>P-value: {row['local_moran_p']:.3f}\"\n",
    "        ).add_to(fg_moran)\n",
    "    \n",
    "    # 3. Cluster Types Layer\n",
    "    for idx, row in gdf_analysis.iterrows():\n",
    "        color = cluster_colors[row['cluster_type']]\n",
    "        folium.GeoJson(\n",
    "            row.geometry.__geo_interface__,\n",
    "            style_function=lambda x, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.7\n",
    "            },\n",
    "            tooltip=f\"Cluster Type: {row['cluster_type']}\"\n",
    "        ).add_to(fg_clusters)\n",
    "    \n",
    "    # Add feature groups to map\n",
    "    fg_original.add_to(m)\n",
    "    fg_moran.add_to(m)\n",
    "    fg_clusters.add_to(m)\n",
    "    \n",
    "    # Add colormaps to map\n",
    "    colormap_original.add_to(m)\n",
    "    colormap_moran.add_to(m)\n",
    "    \n",
    "    # Add cluster type legend\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; bottom: 50px; right: 50px; z-index: 1000; background-color: white; \n",
    "                padding: 10px; border: 2px solid grey; border-radius: 5px\">\n",
    "    <p><strong>Cluster Types</strong></p>\n",
    "    \"\"\"\n",
    "    for cluster_type, color in cluster_colors.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <p><i class=\"fa fa-square fa-1x\" style=\"color:{color}\"></i> {cluster_type}</p>\n",
    "        \"\"\"\n",
    "    legend_html += \"</div>\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results = {\n",
    "        'global_statistics': {\n",
    "            'morans_i': moran.I,\n",
    "            'p_value': moran.p_sim,\n",
    "            'z_score': moran.z_sim\n",
    "        },\n",
    "        'cluster_summary': gdf_analysis['cluster_type'].value_counts().to_dict(),\n",
    "        'local_statistics': {\n",
    "            'mean_local_i': gdf_analysis['local_moran_i'].mean(),\n",
    "            'significant_clusters': (gdf_analysis['local_moran_p'] < 0.05).sum(),\n",
    "            'percent_significant': (gdf_analysis['local_moran_p'] < 0.05).mean() * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "results, interactive_map = analyze_spatial_correlation_interactive(\n",
    "    gdf=census_election_data,\n",
    "    column='religion_american_baptist_churches_in_the_usa_percent_adherents_of_total_adherents'\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print_spatial_correlation_results(results)\n",
    "\n",
    "# Display map\n",
    "interactive_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get all spatial correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from libpysal.weights import Queen, KNN\n",
    "from esda.moran import Moran, Moran_Local\n",
    "import warnings\n",
    "from typing import Union, Tuple, Dict, List\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class SpatialCorrelationResult:\n",
    "    \"\"\"Data class to store correlation results for a single feature\"\"\"\n",
    "    feature: str\n",
    "    global_morans_i: float\n",
    "    global_p_value: float\n",
    "    global_z_score: float\n",
    "    mean_local_i: float\n",
    "    significant_clusters: int\n",
    "    percent_significant: float\n",
    "    cluster_counts: Dict[str, int]\n",
    "\n",
    "def calculate_multi_feature_correlation(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    features: List[str] = None,\n",
    "    weight_type: str = 'queen',\n",
    "    k_neighbors: int = 5,\n",
    "    exclude_columns: List[str] = None\n",
    ") -> List[SpatialCorrelationResult]:\n",
    "    \"\"\"\n",
    "    Calculates spatial correlation metrics for multiple features in a GeoDataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing geometry and data\n",
    "    features : List[str], optional\n",
    "        List of column names to analyze. If None, will analyze all numeric columns\n",
    "    weight_type : str, optional (default='queen')\n",
    "        Type of spatial weights to use ('queen' or 'knn')\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of neighbors for KNN weights\n",
    "    exclude_columns : List[str], optional\n",
    "        List of column names to exclude from analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[SpatialCorrelationResult]\n",
    "        List of correlation results for each feature\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "        \n",
    "    # If no features specified, use all numeric columns\n",
    "    if features is None:\n",
    "        features = gdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove excluded columns\n",
    "    if exclude_columns:\n",
    "        features = [f for f in features if f not in exclude_columns]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Create spatial weights matrix once\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if weight_type.lower() == 'queen':\n",
    "            weights = Queen.from_dataframe(gdf, use_index=True)\n",
    "            # Handle islands\n",
    "            islands = weights.islands\n",
    "            if len(islands) > 0:\n",
    "                gdf = gdf[~gdf.index.isin(islands)].copy()\n",
    "                weights = Queen.from_dataframe(gdf, use_index=True)\n",
    "        elif weight_type.lower() == 'knn':\n",
    "            weights = KNN.from_dataframe(gdf, k=k_neighbors)\n",
    "        else:\n",
    "            raise ValueError(\"weight_type must be either 'queen' or 'knn'\")\n",
    "    \n",
    "    weights.transform = 'r'  # Row-standardize weights\n",
    "    \n",
    "    # Analyze each feature\n",
    "    for feature in features:\n",
    "        # Skip if feature has all missing values\n",
    "        if gdf[feature].isnull().all():\n",
    "            continue\n",
    "            \n",
    "        # Create copy of data without missing values for this feature\n",
    "        gdf_clean = gdf.dropna(subset=[feature]).copy()\n",
    "        \n",
    "        if len(gdf_clean) < 2:  # Skip if not enough data\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Calculate Global Moran's I\n",
    "            moran = Moran(gdf_clean[feature], weights)\n",
    "            \n",
    "            # Calculate Local Moran's I\n",
    "            local_moran = Moran_Local(gdf_clean[feature], weights)\n",
    "            \n",
    "            # Calculate cluster types\n",
    "            sig_mask = local_moran.p_sim < 0.05\n",
    "            std_val = (gdf_clean[feature] - gdf_clean[feature].mean()) / gdf_clean[feature].std()\n",
    "            lag_val = weights.sparse.dot(std_val)\n",
    "            \n",
    "            cluster_types = np.full(len(gdf_clean), 'Not Significant', dtype=object)\n",
    "            cluster_types[sig_mask & (std_val > 0) & (lag_val > 0)] = 'High-High'\n",
    "            cluster_types[sig_mask & (std_val < 0) & (lag_val < 0)] = 'Low-Low'\n",
    "            cluster_types[sig_mask & (std_val > 0) & (lag_val < 0)] = 'High-Low'\n",
    "            cluster_types[sig_mask & (std_val < 0) & (lag_val > 0)] = 'Low-High'\n",
    "            \n",
    "            cluster_counts = dict(pd.Series(cluster_types).value_counts())\n",
    "            \n",
    "            result = SpatialCorrelationResult(\n",
    "                feature=feature,\n",
    "                global_morans_i=moran.I,\n",
    "                global_p_value=moran.p_sim,\n",
    "                global_z_score=moran.z_sim,\n",
    "                mean_local_i=np.mean(local_moran.Is),\n",
    "                significant_clusters=np.sum(sig_mask),\n",
    "                percent_significant=np.mean(sig_mask) * 100,\n",
    "                cluster_counts=cluster_counts\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing feature {feature}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def get_top_correlations(\n",
    "    results: List[SpatialCorrelationResult],\n",
    "    n: int = 5,\n",
    "    p_threshold: float = 0.05,\n",
    "    sort_by: str = 'global'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the top n features by spatial correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : List[SpatialCorrelationResult]\n",
    "        List of correlation results\n",
    "    n : int, optional (default=5)\n",
    "        Number of top features to return\n",
    "    p_threshold : float, optional (default=0.05)\n",
    "        Only include results with p-value below this threshold\n",
    "    sort_by : str, optional (default='global')\n",
    "        Sort by 'global' or 'local' Moran's I\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sorted DataFrame of top correlations\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'feature': r.feature,\n",
    "            'global_morans_i': r.global_morans_i,\n",
    "            'global_p_value': r.global_p_value,\n",
    "            'global_z_score': r.global_z_score,\n",
    "            'mean_local_i': r.mean_local_i,\n",
    "            'significant_clusters': r.significant_clusters,\n",
    "            'percent_significant': r.percent_significant\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    # Filter by p-value\n",
    "    df = df[df['global_p_value'] < p_threshold]\n",
    "    \n",
    "    # Sort by specified metric\n",
    "    sort_col = 'global_morans_i' if sort_by == 'global' else 'mean_local_i'\n",
    "    df = df.sort_values(by=sort_col, ascending=False)\n",
    "    \n",
    "    return df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate correlations for all numeric features\n",
    "# results = calculate_multi_feature_correlation(\n",
    "#     census_election_data,\n",
    "#     weight_type='queen',  # or 'knn'\n",
    "#     k_neighbors=5  # only needed if using 'knn'\n",
    "# )\n",
    "\n",
    "# Get top n features by global Moran's I\n",
    "top_global = get_top_correlations(\n",
    "    results,\n",
    "    n=1000,\n",
    "    p_threshold=0.05,\n",
    "    sort_by='global'\n",
    ")\n",
    "\n",
    "# Get top n features by local Moran's I\n",
    "top_local = get_top_correlations(\n",
    "    results,\n",
    "    n=1000,\n",
    "    p_threshold=0.05,\n",
    "    sort_by='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "top_global.sort_values(by='percent_significant', ascending=False).head(100)\n",
    "# top_local.sort_values(by='mean_local_i', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_election_data.state_name.str.lower().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "def clean_place_name(name, state=None):\n",
    "    \"\"\"\n",
    "    Clean place names for consistent matching.\n",
    "    Now handles state-specific naming conventions.\n",
    "    \n",
    "    Parameters:\n",
    "    name: str, place name to clean\n",
    "    state: str, optional state abbreviation to apply state-specific rules\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    \n",
    "    # Convert to string and clean accented characters\n",
    "    name = str(name).strip()\n",
    "    name = unidecode.unidecode(name)\n",
    "    \n",
    "    # Remove common suffixes based on state\n",
    "    suffixes_to_remove = {\n",
    "        'LA': [' County', ' Parish'],\n",
    "        'AK': [' County', ' Borough', ' Census Area', ' Municipality'],\n",
    "        'VA': [' County', ' city'],  # Virginia has independent cities\n",
    "        'DC': [' County'],  # Handle DC separately\n",
    "        None: [' County', ' Parish', ' Borough', ' Census Area', ' Municipality', ' city']\n",
    "    }\n",
    "    \n",
    "    # Get the appropriate list of suffixes to remove\n",
    "    suffixes = suffixes_to_remove.get(state, suffixes_to_remove[None])\n",
    "    \n",
    "    # Remove any matching suffix\n",
    "    for suffix in suffixes:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    # Standardize specific terms\n",
    "    replacements = {\n",
    "        'Municipality': 'Municipio',\n",
    "        'City and Borough': 'Borough',\n",
    "        'Census Area': 'Census Area',\n",
    "        ' city': ' City',\n",
    "        'LaSalle': 'La Salle',\n",
    "        'LaPorte': 'La Porte',\n",
    "        'DeSoto': 'De Soto',\n",
    "        'DeKalb': 'De Kalb',\n",
    "        'St.': 'St',\n",
    "        'Ste.': 'Ste'\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "    \n",
    "    # Add back the appropriate suffix based on state\n",
    "    if state == 'LA':\n",
    "        name = name + ' Parish'\n",
    "    elif state == 'AK':\n",
    "        # Need to check original name to determine correct suffix\n",
    "        if ' Borough' in str(name):\n",
    "            name = name + ' Borough'\n",
    "        elif ' Census Area' in str(name):\n",
    "            name = name + ' Census Area'\n",
    "        else:\n",
    "            name = name  # No suffix for municipalities\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "def add_fips_codes_to_election_data(election_df, fips_csv_path):\n",
    "    \"\"\"\n",
    "    Add FIPS codes to election data by matching county and state.\n",
    "    \n",
    "    Parameters:\n",
    "    election_df (pandas.DataFrame): Election dataframe with State and County columns\n",
    "    fips_csv_path (str): Path to the FIPS CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Election dataframe with FIPS codes added\n",
    "    \"\"\"\n",
    "    # Read FIPS data\n",
    "    fips_df = pd.read_csv(fips_csv_path)\n",
    "    \n",
    "    # Filter out non-place rows\n",
    "    fips_df = fips_df[~fips_df['name'].isna()]\n",
    "    fips_df = fips_df[fips_df['name'].str.contains('County|Borough|Parish|Census Area|city|Municipio', \n",
    "                                                  case=False, na=False)]\n",
    "    \n",
    "    # Create state abbreviation mapping\n",
    "    state_abbr = pd.read_csv('https://raw.githubusercontent.com/jasonong/List-of-US-States/master/states.csv')\n",
    "    state_map = dict(zip(state_abbr['State'], state_abbr['Abbreviation']))\n",
    "    \n",
    "    # Add state abbreviations to election data\n",
    "    election_df = election_df.copy()\n",
    "    election_df['state_abbr'] = election_df['State'].map(state_map)\n",
    "    \n",
    "    # Clean up names in both dataframes, using state information\n",
    "    election_df['clean_county'] = election_df.apply(\n",
    "        lambda x: clean_place_name(x['County'], x['state_abbr']), axis=1\n",
    "    )\n",
    "    fips_df['clean_name'] = fips_df.apply(\n",
    "        lambda x: clean_place_name(x['name'], x['state']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Merge the dataframes\n",
    "    result = pd.merge(\n",
    "        election_df,\n",
    "        fips_df[['fips', 'clean_name', 'state']],\n",
    "        how='left',\n",
    "        left_on=['clean_county', 'state_abbr'],\n",
    "        right_on=['clean_name', 'state']\n",
    "    )\n",
    "    \n",
    "    # Drop working columns\n",
    "    result = result.drop(['clean_county', 'clean_name', 'state'], axis=1)\n",
    "    \n",
    "    # Convert FIPS codes to strings with leading zeros\n",
    "    result['fips'] = result['fips'].astype(str).str.zfill(5)\n",
    "    \n",
    "    # Check for unmatched counties\n",
    "    unmatched = result[result['fips'].isna()]\n",
    "    if len(unmatched) > 0:\n",
    "        print(f\"\\nWarning: {len(unmatched)} counties could not be matched with FIPS codes:\")\n",
    "        for _, row in unmatched.iterrows():\n",
    "            print(f\"- {row['County']}, {row['State']}\")\n",
    "    \n",
    "    # Reorder columns to put FIPS code after State\n",
    "    cols = list(result.columns)\n",
    "    state_idx = cols.index('State')\n",
    "    cols.remove('fips')\n",
    "    cols.insert(state_idx + 1, 'fips')\n",
    "    result = result[cols]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the 2024 results csv, may have weird encoding so use latin1\n",
    "# results_2024 = pd.read_csv('../../data/election/2024_county_results_general.csv')\n",
    "results_2024 = pd.read_csv('../../data/election/2024_county_results_general.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that contain a the text 'Est' in the County column\n",
    "results_2024 = results_2024[~results_2024['County'].str.contains('Est')]\n",
    "# drop cols with title Robert F. Kennedy Jr\tChase Oliver\tJill Stein\n",
    "results_2024 = results_2024.drop(columns=['Robert F. Kennedy Jr', 'Chase Oliver', 'Jill Stein'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that have '-' in any column\n",
    "results_2024 = results_2024[~results_2024.isin(['-']).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the percent columns to floats (they are currently strings with percent signs) (the cols are Kamala Harris and Donald Trump)\n",
    "percent_columns = ['Kamala Harris', 'Donald Trump']\n",
    "results_2024[percent_columns] = results_2024[percent_columns].replace('%', '', regex=True).astype(float)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ' County' to the end of every value in the County column\n",
    "results_2024['County'] = results_2024['County'] + ' County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your election dataframe is called 'election_df'\n",
    "fips_path = '../../data/election/fips_state_and_county.csv'\n",
    "votes_2024_fips = add_fips_codes_to_election_data(results_2024, fips_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the titles of Kamala Harris,\tDonald Trump to 'per_dem_2024', 'per_gop_2024'\n",
    "votes_2024_fips = votes_2024_fips.rename(columns={'Kamala Harris': 'per_dem_2024', 'Donald Trump': 'per_gop_2024'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votes_2024_fips\n",
    "# drop all rows where fips is '00nan'\n",
    "votes_2024_fips_fixed = votes_2024_fips[votes_2024_fips['fips'] != '00nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fips to float\n",
    "votes_2024_fips_fixed['fips'] = votes_2024_fips_fixed['fips'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_2024_fips_fixed.fips.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census_election_data.fips\n",
    "# merge the votes_2024_fips dataframe with the census_election_data dataframe on the fips column\n",
    "merged_data = pd.merge(census_election_data, votes_2024_fips_fixed, on='fips', how='left')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop per_dem_2024\tper_gop_2024\tpresident_winner_2024\tflipped_2016\tflipped_2020\tflipped_2024\twinner_2024\twinner_2024_numeric\tflipped_2012_2016\tflipped_2016_2020\tflipped_2020_2024\tflipped_dem_2016\tflipped_gop_2016\tflipped_dem_2020\tflipped_gop_2020\tflipped_dem_2024\tflipped_gop_2024 from census_election_data\n",
    "census_election_data = census_election_data.drop(columns=['per_dem_2024', 'per_gop_2024', 'president_winner_2024', 'flipped_2016', 'flipped_2020', 'flipped_2024', 'winner_2024', 'winner_2024_numeric', 'flipped_2012_2016', 'flipped_2016_2020', 'flipped_2020_2024', 'flipped_dem_2016', 'flipped_gop_2016', 'flipped_dem_2020', 'flipped_gop_2020', 'flipped_dem_2024', 'flipped_gop_2024'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename state_abbr_x to state_abbr, drop state_abbr_y, State, County\n",
    "merged_data = merged_data.rename(columns={'state_abbr_x': 'state_abbr'})\n",
    "merged_data = merged_data.drop(columns=['state_abbr_y', 'State', 'County'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged_data, add a column called winner_2024_numeric that is 1 if per_gop_2024 > per_dem_2024, 0 otherwise\n",
    "merged_data['winner_2024_numeric'] = np.where(merged_data['per_gop_2024'] > merged_data['per_dem_2024'], 1, 0)\n",
    "# in merged_data, add a column called president_winner_2024 that is 'GOP'\n",
    "merged_data['president_winner_2024'] = 'GOP'\n",
    "# in merged_data, add a column called winner_2024 that is 'GOP' if winner_2024_numeric is 1, 'DEM' otherwise\n",
    "merged_data['winner_2024'] = np.where(merged_data['winner_2024_numeric'] == 1, 'GOP', 'DEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged_data, add a column called state_electoral_votes that is the number of electoral votes for the state\n",
    "state_electoral_votes = {\n",
    "    'AL': 9,\n",
    "    'AK': 3,\n",
    "    'AZ': 11,\n",
    "    'AR': 6,\n",
    "    'CA': 55,\n",
    "    'CO': 9,\n",
    "    'CT': 7,\n",
    "    'DE': 3,\n",
    "    'DC': 3,\n",
    "    'FL': 29,\n",
    "    'GA': 16,\n",
    "    'HI': 4,\n",
    "    'ID': 4,\n",
    "    'IL': 20,\n",
    "    'IN': 11,\n",
    "    'IA': 6,\n",
    "    'KS': 6,\n",
    "    'KY': 8,\n",
    "    'LA': 8,\n",
    "    'ME': 4,\n",
    "    'MD': 10,\n",
    "    'MA': 11,\n",
    "    'MI': 16,\n",
    "    'MN': 10,\n",
    "    'MS': 6,\n",
    "    'MO': 10,\n",
    "    'MT': 3,\n",
    "    'NE': 5,\n",
    "    'NV': 6,\n",
    "    'NH': 4,\n",
    "    'NJ': 14,\n",
    "    'NM': 5,\n",
    "    'NY': 29,\n",
    "    'NC': 15,\n",
    "    'ND': 3,\n",
    "    'OH': 18,\n",
    "    'OK': 7,\n",
    "    'OR': 7,\n",
    "    'PA': 20,\n",
    "    'RI': 4,\n",
    "    'SC': 9,\n",
    "    'SD': 3,\n",
    "    'TN': 11,\n",
    "    'TX': 38,\n",
    "    'UT': 6,\n",
    "    'VT': 3,\n",
    "    'VA': 13,\n",
    "    'WA': 12,\n",
    "    'WV': 5,\n",
    "    'WI': 10,\n",
    "    'WY': 3\n",
    "}\n",
    "\n",
    "merged_data['state_electoral_votes'] = merged_data['state_abbr'].map(state_electoral_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged_data, create a column called state_pop_approx that is the total population of the state.  use the Total:_2020 column as the county population, and sum these values for each state\n",
    "state_pop_approx = merged_data.groupby('state_abbr')['Total:_2020'].sum()\n",
    "merged_data['state_pop_approx'] = merged_data['state_abbr'].map(state_pop_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged_data, create a column called pct_pop_state that is the Total:_2020 divided by the sum of state_pop_approx\n",
    "merged_data['pct_pop_state'] = merged_data['Total:_2020'] / merged_data['state_pop_approx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 10 rows where pct_pop_state is highest\n",
    "merged_data.sort_values('pct_pop_state', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged data, create a double histogram where the left side is the distribution of pct_pop_state where Metro_2013 is 0, and the right side is the distribution of pct_pop_state where Metro_2013 is 1\n",
    "# plot them on the same axis so they can be easily compared\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Filter data for each histogram\n",
    "metro_0 = merged_data[merged_data['Metro_2013'] == 0]\n",
    "metro_1 = merged_data[merged_data['Metro_2013'] == 1]\n",
    "\n",
    "# Plot histograms\n",
    "sns.histplot(metro_0['pct_pop_state'], bins=20, color='skyblue', label='Non-Metro', ax=ax)\n",
    "sns.histplot(metro_1['pct_pop_state'], bins=20, color='salmon', label='Metro', ax=ax)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Population Percentage')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Population Percentage by Metro Status')\n",
    "plt.legend(title='Metro Status')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in merged_data, show col names that contain 'president'\n",
    "president_columns = [col for col in merged_data.columns if 'flipped' in col]\n",
    "president_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop(columns=president_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flips(merged_data):\n",
    "    \"\"\"\n",
    "    Calculate party flips between election cycles.\n",
    "    3 = DEM to GOP flip (0 to 1)\n",
    "    2 = GOP to DEM flip (1 to 0)\n",
    "    0 = No flip (stayed same party)\n",
    "    \"\"\"\n",
    "    # 2012 to 2016 flips\n",
    "    merged_data['flipped_2012_2016'] = 0  # Initialize with no flip\n",
    "    dem_to_gop_mask = (merged_data['winner_2012_numeric'] == 0) & (merged_data['winner_2016_numeric'] == 1)\n",
    "    gop_to_dem_mask = (merged_data['winner_2012_numeric'] == 1) & (merged_data['winner_2016_numeric'] == 0)\n",
    "    merged_data.loc[dem_to_gop_mask, 'flipped_2012_2016'] = 3\n",
    "    merged_data.loc[gop_to_dem_mask, 'flipped_2012_2016'] = 2\n",
    "    \n",
    "    # 2016 to 2020 flips\n",
    "    merged_data['flipped_2016_2020'] = 0  # Initialize with no flip\n",
    "    dem_to_gop_mask = (merged_data['winner_2016_numeric'] == 0) & (merged_data['winner_2020_numeric'] == 1)\n",
    "    gop_to_dem_mask = (merged_data['winner_2016_numeric'] == 1) & (merged_data['winner_2020_numeric'] == 0)\n",
    "    merged_data.loc[dem_to_gop_mask, 'flipped_2016_2020'] = 3\n",
    "    merged_data.loc[gop_to_dem_mask, 'flipped_2016_2020'] = 2\n",
    "    \n",
    "    # 2020 to 2024 flips\n",
    "    merged_data['flipped_2020_2024'] = 0  # Initialize with no flip\n",
    "    dem_to_gop_mask = (merged_data['winner_2020_numeric'] == 0) & (merged_data['winner_2024_numeric'] == 1)\n",
    "    gop_to_dem_mask = (merged_data['winner_2020_numeric'] == 1) & (merged_data['winner_2024_numeric'] == 0)\n",
    "    merged_data.loc[dem_to_gop_mask, 'flipped_2020_2024'] = 3\n",
    "    merged_data.loc[gop_to_dem_mask, 'flipped_2020_2024'] = 2\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = calculate_flips(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts of each type of flip for each period\n",
    "for col in ['flipped_2012_2016', 'flipped_2016_2020', 'flipped_2020_2024']:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(merged_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged_data to geojson ('../../data/election/final_data/county_demographics_with_elections_2012_2024.geojson')\n",
    "merged_data.to_file('../../data/election/final_data/county_demographics_with_elections_2012_2024.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
